{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports and database creation\n",
    "from ftplib import FTP\n",
    "import time\n",
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "import sqlite3\n",
    "import re\n",
    "from ete3 import PhyloTree, Tree\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from ete3.coretype.tree import TreeError\n",
    "import itertools\n",
    "from Bio.Phylo.PAML import codeml\n",
    "import requests, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp, log, log10\n",
    "from scipy.stats import chi2, mannwhitneyu, spearmanr, combine_pvalues\n",
    "from statsmodels.stats.multitest import fdrcorrection as fdr\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "db = sqlite3.connect('drosophilaDatabase_diptera')\n",
    "db.isolation_level = None\n",
    "cursor = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6989dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data fetching and processing\n",
    "#connect to ftp server\n",
    "ftp = FTP('ftp.ncbi.nlm.nih.gov')\n",
    "ftp.login()\n",
    "#cycle through all species\n",
    "print('Connected to site successfully')\n",
    "#connect to ftp site, loop over file locations for each species\n",
    "with open('drosophilaProjectNcbiFtpFileLocations.txt','r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip('\\n').split('\\t')\n",
    "        spAbb, path = line\n",
    "        #fetch raw files for CDS, translated CDS, gff\n",
    "        outputFileTrans = 'dipteraTranslations_raw/'+spAbb + '_raw_trans.faa.gz'\n",
    "        outputFileCDS = 'dipteraCDS_raw/'+spAbb + '_raw_cds.fna.gz'\n",
    "        outputFileGFF = 'dipteraGFF_raw/'+spAbb + '_raw.gff.gz'\n",
    "        print('Species:',spAbb)\n",
    "        fullPath = '/genomes/all/' + path\n",
    "        ftp.cwd(fullPath) #change to relevant directory for this species\n",
    "        #navigate to correct files and save to correct file names\n",
    "        fileList = []\n",
    "        if 'GC' in fullPath: #files located in GCA or GCF rather than in annotation_releases, navigation a little different\n",
    "            ftp.retrlines('NLST',callback=lambda x: fileList.append(x))\n",
    "            faaFile = [file for file in fileList if 'translated_cds' in file][0]\n",
    "            fnaFile = [file for file in fileList if 'cds_from_genomic' in file][0]\n",
    "            gffFile = [file for file in fileList if 'genomic.gff' in file][0]\n",
    "            print('Transferring files...')\n",
    "            t = time.time()\n",
    "            with open(outputFileTrans, 'wb') as outT:\n",
    "                ftp.retrbinary('RETR '+faaFile, outT.write)\n",
    "            with open(outputFileCDS, 'wb') as outC:\n",
    "                ftp.retrbinary('RETR '+fnaFile, outC.write)\n",
    "            with open(outputFileGFF, 'wb') as outG:\n",
    "                ftp.retrbinary('RETR '+gffFile, outG.write)\n",
    "            print('Done! ('+str(round(time.time()-t,1))+' seconds)')\n",
    "            print()\n",
    "        else:\n",
    "            ftp.retrlines('LIST',callback=lambda x: fileList.append(x)) #callback arg is a function applied to each line returned\n",
    "            directory = [x for x in fileList if x.startswith('d')][0].split(' ')[-1]\n",
    "            ftp.cwd(directory)\n",
    "            fileList2 = []\n",
    "            ftp.retrlines('NLST',callback=lambda x: fileList.append(x))\n",
    "            faaFile = [file for file in fileList if 'translated_cds' in file][0]\n",
    "            fnaFile = [file for file in fileList if 'cds_from_genomic' in file][0]\n",
    "            gffFile = [file for file in fileList if 'genomic.gff' in file][0]\n",
    "            print('Transferring files...')\n",
    "            t = time.time()\n",
    "            with open(outputFileTrans, 'wb') as outT:\n",
    "                ftp.retrbinary('RETR '+faaFile, outT.write)\n",
    "            with open(outputFileCDS, 'wb') as outC:\n",
    "                ftp.retrbinary('RETR '+fnaFile, outC.write)\n",
    "            with open(outputFileGFF, 'wb') as outG:\n",
    "                ftp.retrbinary('RETR '+gffFile, outG.write)\n",
    "            print('Done! ('+str(round(time.time()-t,1))+' seconds)')\n",
    "            print()\n",
    "print('All done! Goodbye.')\n",
    "print()\n",
    "ftp.close()\n",
    "print('Processing protein seq files...')\n",
    "dictDict = {}\n",
    "for file in os.listdir('./dipteraTranslations_raw'):\n",
    "    dictDict[file] = {}\n",
    "    filename = './dipteraTranslations_raw/' + file\n",
    "    print(filename)\n",
    "    #unzip to stdout and pipe to here, faster and more space efficient than having an uncompressed file\n",
    "    unzipP = Popen(['gunzip','-c',filename],stdout=PIPE)\n",
    "    text,err = unzipP.communicate()\n",
    "    text = text.decode()\n",
    "    splitText = text.split('\\n')\n",
    "    for i, line in enumerate(splitText):\n",
    "        if line.startswith('>'): #new header\n",
    "            seq = ''\n",
    "            seqOK = True\n",
    "            if 'locus_tag' in line and 'gene=' in line:\n",
    "                gene = re.search('\\[gene=(.*)\\] \\[locus_tag.*\\]',line).group(1)\n",
    "                prot = re.search('\\[protein_id=(.*)\\] \\[location=',line).group(1)\n",
    "            elif 'gene=' not in line and 'GeneID' in line:\n",
    "                gene = re.search('GeneID:(\\d*)\\] \\[protein.*\\]',line).group(1)\n",
    "                prot = re.search('\\[protein_id=(.*)\\] \\[location=',line).group(1)\n",
    "            \n",
    "            elif 'gene=' in line:\n",
    "                gene = re.search('\\[gene=(.*)\\] \\[db_xref.*\\]',line).group(1)\n",
    "                prot = re.search('\\[protein_id=(.*)\\] \\[location=',line).group(1)\n",
    "            elif 'locus_tag=' in line: #for CCAP, and any otehers with no gene ids\n",
    "                gene = re.search('\\[locus_tag=(.*?)\\]',line).group(1)\n",
    "                try:\n",
    "                    prot = re.search('\\[protein_id=(.*?)\\]',line).group(1)\n",
    "                except AttributeError: #no protein id given\n",
    "                    seqOK = False\n",
    "            else:\n",
    "                print('??')\n",
    "                print(line)\n",
    "        elif i+1 == len(splitText) and seqOK == True: #last line of file, same as last seq but I don't want to deal with index errors\n",
    "            seq = seq+line #add last line\n",
    "            pLen = len(seq)\n",
    "               # print('last line')\n",
    "            try:\n",
    "                if dictDict[file][gene][1] < pLen:\n",
    "                    dictDict[file][gene] = (prot,pLen,seq)\n",
    "            except KeyError:\n",
    "                dictDict[file][gene] = (prot,pLen,seq)\n",
    "        elif splitText[i+1].startswith('>') and seqOK == True: #last sequence line for this protein\n",
    "                #print('last of this prot')\n",
    "            seq = seq+line #add this last line\n",
    "            pLen = len(seq)\n",
    "            try:\n",
    "                if dictDict[file][gene][1] < pLen: #if the current protein for this gene is shorter than this one\n",
    "                    dictDict[file][gene] = (prot,pLen,seq)\n",
    "            except KeyError:\n",
    "                dictDict[file][gene] = (prot,pLen,seq)\n",
    "        elif seqOK == True:\n",
    "            seq = seq+line\n",
    "    print('Writing to output file')\n",
    "    outpath = './dipteraTranslations_processed/'+file.strip('_raw_trans.faa.gz') + '.longest_only.faa'\n",
    "    sp = file.strip('_raw_trans.faa.gz')\n",
    "    print(sp)\n",
    "    pLenDict = dictDict[file]\n",
    "    #write longest sequence and reduced headers to file\n",
    "    with open(outpath,'w') as out:\n",
    "        for gene in pLenDict:\n",
    "            prot, length, sequence = pLenDict[gene]\n",
    "            line = '>' + gene + '|' + prot + '|' + sp + '\\n'\n",
    "            out.write(line)\n",
    "            out.write(sequence+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e562e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating orthogroups table\n",
    "#similar for all tools, only Orthofinder shown\n",
    "#iterate over orthogroups making sure they're complete\n",
    "#I think the latest version has fixed the issue where the groups don't necessarily reflect the orthologues but just in case\n",
    "orthoGroupsReal = []\n",
    "cursor.execute('CREATE TABLE groups_Orthofinder(groupID INTEGER, groupMembers TEXT)')\n",
    "for d in os.listdir('dipteraTranslations_processed/Orthofinder/Results_Mar30/Orthologues/'):\n",
    "    print('Current directory:',d)\n",
    "    for filename in os.listdir('neopTranslations_processed/Orthofinder/Results_Mar30/Orthologues/'+d):\n",
    "        print('   Doing', filename)\n",
    "        path = 'dipteraTranslations_processed/Orthofinder/Results_Mar30/Orthologues/' + d + '/' + filename\n",
    "\n",
    "        with open(path,'r') as file:\n",
    "            file.readline()\n",
    "            for line in file:\n",
    "                group, focal, other = line.strip('\\n').split('\\t')\n",
    "                focal = focal.split(', ')\n",
    "                other = other.split(', ')\n",
    "                wholeGroup = focal\n",
    "                wholeGroup.extend(other) #entire group that are orthologous according to this one line\n",
    "                found = False\n",
    "                for oldGroup in orthoGroupsReal:\n",
    "                    if not set(wholeGroup).isdisjoint(set(oldGroup)): #if has things in common\n",
    "                        #add on all new genes\n",
    "                        oldGroup.extend(list(set(wholeGroup)-set(oldGroup)))\n",
    "                        found = True\n",
    "                        break #exit once have found the existing group\n",
    "                if not found: #no matching group found, add the group for this line to the search\n",
    "                    orthoGroupsReal.append(wholeGroup)\n",
    "    print()\n",
    "    print(dCount,'directories done')\n",
    "    print()\n",
    "for i, group in enumerate(orthoGroupsReal):\n",
    "    cursor.execute('INSERT INTO groups_Orthofinder VALUES(?,?)',(i,','.join(group)))\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of singletons from blast output (all v all blastp, filtered to only hits with E value under 0.1)\n",
    "with open('dsuz.filtered.out', 'r') as file:\n",
    "    gL = []\n",
    "    for line in file:\n",
    "        line = line.strip().split('\\t')\n",
    "        gL.append(line[0])\n",
    "sing_list = []\n",
    "for x in set(gL):\n",
    "#     if only one hit for a given gene, it must be a singleton (only hit that meets cutoff is self)\n",
    "    if gL.count(x) == 1:\n",
    "        sing_list.append(x)\n",
    "print('Total singletons to be considered:',len(sing_list))\n",
    "with open('dsuz_singletons.txt','w') as file:\n",
    "    for p in sing_list:\n",
    "        file.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating sequence table for easier fetching later\n",
    "c = 1\n",
    "cursor.execute('CREATE TABLE IF NOT EXISTS sequenceTab(id TEXT,seq TEXT)')\n",
    "for file in os.listdir('dipteraTranslations_processed/'):\n",
    "    if not file.endswith('.faa'):\n",
    "        continue\n",
    "    print('Species',str(c),'of 37:',file)\n",
    "    fullPath = 'dipteraTranslations_processed/'+file\n",
    "    inputList = []\n",
    "    with open(fullPath,'r') as fasta:\n",
    "        seq = ''\n",
    "        for line in fasta:\n",
    "            line = line.strip('\\n')\n",
    "            if line.startswith('>') and seq == '':\n",
    "                i = line.strip('>')\n",
    "                print(seq)\n",
    "            elif line.startswith('>'):\n",
    "                inputList.append((i,seq))\n",
    "                i = line.strip('>')\n",
    "                seq = ''\n",
    "            else:\n",
    "                seq = seq + line\n",
    "                \n",
    "        cursor.executemany('INSERT INTO sequenceTab VALUES(?,?)',(inputList))\n",
    "        inputList = []\n",
    "        c += 1\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('UPDATE sequenceTab SET id = REPLACE(id,\"(\",\"_\")')\n",
    "cursor.execute('UPDATE sequenceTab SET id = REPLACE(id,\")\",\"_\")')\n",
    "cursor.execute('UPDATE sequenceTab SET id = REPLACE(id,\":\",\"_\")')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSingGroups(groupTable,singList):\n",
    "#     obtain the correct groups for the singleton set\n",
    "    realGroupDict = {}\n",
    "    cursor.execute('SELECT * FROM '+groupTable)\n",
    "    for a,b in cursor.fetchall():\n",
    "        realGroupDict[a] = b.split(',')\n",
    "    for s in singList:\n",
    "        try:\n",
    "            group = [x for x in realGroupDict if s in realGroupDict[x]][0]\n",
    "            yield group\n",
    "        except IndexError: #no group found\n",
    "            try:\n",
    "                group = [x for x in realGroupDict if s in ','.join(realGroupDict[x])][0]\n",
    "                yield group\n",
    "            except IndexError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGroupFastas(groupGen,outputFastaDir):\n",
    "    count, ti = 0, time.time()\n",
    "    #create neopTranslations_by_orthogroup fastas\n",
    "    #for each group fetch the sequences and add to file\n",
    "    if not os.path.exists(outputFastaDir):\n",
    "        os.makedirs(outputFastaDir)\n",
    "    print('Creating fastas for singleton orthogroups...')\n",
    "    for group in groupGen:\n",
    "        groupFasta = outputFastaDir + '/group'+str(group)+'.fa'\n",
    "        if os.path.exists(groupFasta):\n",
    "            continue\n",
    "        with open(groupFasta,'w') as out:\n",
    "#         orthoGroupList = []\n",
    "#         unAssigned = []\n",
    "            cursor.execute('SELECT members FROM '+groupTable+' WHERE group == ?',(group,))\n",
    "            geneList = cursor.fetchall()[0][0].split(',')\n",
    "            \n",
    "            for g in geneList:\n",
    "                if g == '*':\n",
    "                    continue\n",
    "                if '(' in g and ')' in g:\n",
    "                    g = g.replace('(','_')\n",
    "                    g = g.replace(')','_')\n",
    "                cursor.execute('SELECT seq FROM sequenceTab WHERE id == ?',(g,))\n",
    "                try:\n",
    "                    seq = cursor.fetchall()[0][0]\n",
    "                except IndexError:\n",
    "                    print(g, 'seems to be missing in seq table,from group',group)\n",
    "                    raise ValueError\n",
    "                out.write('>'+g+'\\n')\n",
    "                out.write(seq+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doGroupAlignment(alignDirPath,fastaDirPath,singList=None,groupTable=None):\n",
    "    count, ti = 0,time.time()\n",
    "    if not singList:\n",
    "        for f in os.listdir(fastaDirPath):\n",
    "            fullInFastaPath = fastaDirPath + '/' + f\n",
    "            outFasta = alignDirPath + '/' + f[:-3] + '_alignment.fa'\n",
    "            if os.path.exists(outFasta):\n",
    "                continue\n",
    "            cline = ['muscle','-in', fullInFastaPath, '-out', outFasta]\n",
    "            p = Popen(cline, stdout=PIPE, stderr=PIPE)\n",
    "            out, err = p.communicate()\n",
    "            count += 1\n",
    "            if count % 200 == 0:\n",
    "                print(count, 'done', round(time.time()-ti, 2), 'seconds')\n",
    "    else:\n",
    "        #using the fastas output by OMA so will need to filter to only align relevant groups\n",
    "        groupList = extractSingGroups(groupTable,singList)\n",
    "        for group in groupList:\n",
    "            fullInFastaPath = fastaDirPath+ '/' + 'OG'+group[3:] + '.fa'\n",
    "            outFasta = alignDirPath+ '/' + group + '_alignment.fa'\n",
    "            cline = ['muscle','-in', fullInFastaPath, '-out', outFasta]\n",
    "            p = Popen(cline, stdout=PIPE, stderr=PIPE)\n",
    "            out, err = p.communicate()\n",
    "            count += 1\n",
    "            if count % 200 == 0:\n",
    "                print(count, 'done', round(time.time()-ti, 2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrees(alignDirPath):\n",
    "    count = 0\n",
    "    t = time.time()\n",
    "    allFiles = [x for x in os.listdir(alignDirPath)]\n",
    "    toDo = []\n",
    "    for file in allFiles:\n",
    "        if file.endswith('.fa') and file +'.log' not in allFiles:\n",
    "            toDo.append(file)\n",
    "    print('Trees to build, total:',len(toDo))\n",
    "    print('Starting ...')\n",
    "    for al in toDo:\n",
    "        alFile = 'neopTranslations_alignments_current/' + al\n",
    "        cmd = ['../SOFTWARE/iqtree-1.6.12-MacOSX/bin/iqtree', '-s', alFile, '-bb', str(1000),'-nt','3', '-mset','WAG,LG,JTT']\n",
    "        p = Popen(cmd,stdout=PIPE, stderr = PIPE)\n",
    "        out,err = p.communicate()\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(count, 'done:', time.time()-t, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_species(name):\n",
    "    return(name[-4:])\n",
    "def processTrees(treeDirPath,singList,outTable,groupTable): #needs review?\n",
    "    cursor.execute('CREATE TABLE IF NOT EXISTS '+outTable+'(id TEXT, orthogroup TEXT, realGroup TEXT, baseTree BLOB, prunedTree BLOB, excludedReason TEXT, split TEXT)')\n",
    "    cursor.execute('SELECT * FROM '+groupTable)\n",
    "    realGroupDict = dict(cursor.fetchall())\n",
    "    for g in singList:\n",
    "        try:\n",
    "            realGroup = [x for x in realGroupDict if g in realGroupDict[x]][0]\n",
    "            if str(realGroup).startswith('OMA'):\n",
    "                treeFile = treeDirPath + '/' + str(realGroup) + '_alignment.fa.treefile'\n",
    "                \n",
    "            else:\n",
    "                treeFile = treeDirPath + '/group'+str(realGroup) + '_alignment.fa.treefile'\n",
    "            orthoGroup = realGroup\n",
    "            with open(treeFile,'r') as treefile:\n",
    "                tree = treefile.readline().strip('\\n')\n",
    "            tree = PhyloTree(tree, format=1) #hopefully\n",
    "            for n in tree.traverse():\n",
    "                n.set_species_naming_function(get_species)\n",
    "            prefList = [['BCOP','AALB','ASTE','AAEG','AEAL','CQUI','CPIP'],\n",
    "                    ['HILL'],['CCAP','BTRY'],['SLEB'],['DBUS','DALB','DVIR','DNOV','DHYD'],\n",
    "                    ['DSUB','DGUA','DPSE','DPER','DMIR'],['DANA'],['DSER','DKIK'],\n",
    "                    ['DELE','DFIC'],['DSUP','DSUZ','DBIA'],['DEUG'],['DERE','DYAK','DSAN']]\n",
    "            for clade in prefList:\n",
    "                outNodes = []\n",
    "                for sp in clade:\n",
    "                    for node in tree.traverse():\n",
    "                        if sp in node.name:\n",
    "                            outNodes.append(node)\n",
    "                if outNodes == []: #that clade doesn't exist in this tree\n",
    "                    continue\n",
    "                elif len(outNodes) == 1: #single species, set as outgroup\n",
    "                    tree.set_outgroup(outNodes[0])\n",
    "                    break\n",
    "                else:\n",
    "                    rootNode = tree.get_common_ancestor(outNodes)\n",
    "                    try:\n",
    "                        tree.set_outgroup(rootNode)\n",
    "                        break\n",
    "                    except TreeError: #error where it's trying to root on it's current root node and can't, \n",
    "                        #it's because the outgroup is split so the common anc isn't a separate node\n",
    "                        #the workaround is to root on some not-outgroup node so they get forced into a clade, \n",
    "                        #then root again using the correct node (which should now exist)\n",
    "                        for node in tree.traverse():\n",
    "                            #filter out nodes that aren't tips i.e. internal nodes, may be '' or bootstrap value\n",
    "                            if get_species(node.name) not in clade and not node.name.isnumeric() and not node.name =='':\n",
    "                                otherNode = node\n",
    "                                break\n",
    "                        #runs into problems when the only species present are in the clade being used to root\n",
    "                        #those trees aren't going to be useful anyways\n",
    "                        else:\n",
    "#                             print('Gene',g,'may have an unrootable tree.')\n",
    "                            continue\n",
    "                        tree.set_outgroup(otherNode)\n",
    "                        rootNode = tree.get_common_ancestor(outNodes)\n",
    "                        tree.set_outgroup(rootNode)\n",
    "                        break\n",
    "            else: #it should break out of the loop if it finds a suitable clade\n",
    "                raise ValueError('Could not find suitable root')\n",
    "            baseTree = str(tree.write())\n",
    "            #pruning tree to relevant species\n",
    "            pList = []\n",
    "            nodes = ['DBIA','DEUG','DMEL','DSIM','DMAU','DERE','DSEC','DYAK','DSUZ','DSAN','DSUP']\n",
    "            for n in nodes:\n",
    "                for node in tree.traverse():\n",
    "                    if n in node.name:\n",
    "                         pList.append(node.name)\n",
    "            try:\n",
    "                tree.prune(pList, preserve_branch_length=True)\n",
    "            except:\n",
    "                print('pruning failed')\n",
    "                print(tree)\n",
    "            pTree = str(tree.write())\n",
    "            exclude = None\n",
    "        except IndexError as e:\n",
    "    #         print(e, 'group not found')\n",
    "            realGroup = None\n",
    "            tree = None\n",
    "            baseTree = None \n",
    "            pTree = None\n",
    "            exclude = 'Not assigned to group'\n",
    "        except FileNotFoundError as e:\n",
    "    #         print(e, 'group not found')\n",
    "            orthoGroup = None\n",
    "            realGroup = None\n",
    "            tree = None\n",
    "            baseTree = None \n",
    "            pTree = None\n",
    "            exclude = 'Not assigned to group/tree could not be built from group'\n",
    "        except ValueError as e:\n",
    "            orthoGroup = None\n",
    "            realGroup = None\n",
    "            tree = None\n",
    "            baseTree = None \n",
    "            pTree = None\n",
    "            exclude = 'Tree not rooted'\n",
    "        cursor.execute('INSERT INTO '+outTable+' VALUES(?,?,?,?,?,?,?)',(g,orthoGroup,realGroup,baseTree,pTree,exclude,None))\n",
    "db.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMissingSp(table):\n",
    "    somethingMissing = 0\n",
    "    cursor.execute('SELECT id,prunedTree FROM '+table+' WHERE excludedReason IS NULL')\n",
    "    geneTreePrunedDict = dict(cursor.fetchall())\n",
    "    for g in geneTreePrunedDict:\n",
    "        tree = PhyloTree(geneTreePrunedDict[g])\n",
    "        for n in tree.traverse():\n",
    "            n.set_species_naming_function(get_species)\n",
    "        sup, bia, suz, eug = False,False,False,False\n",
    "        for node in tree.traverse():\n",
    "            if 'DSUP' in node.name:\n",
    "                sup = True\n",
    "            elif 'DBIA' in node.name:\n",
    "                bia = True\n",
    "            elif 'DSUZ' in node.name:\n",
    "                suz = True\n",
    "            elif 'DEUG' in node.name:\n",
    "                eug = True\n",
    "        if all([sup,bia,suz,eug]):\n",
    "            singTreeListPrunedMissingChecked.append(g)\n",
    "        else:\n",
    "            cursor.execute('UPDATE '+table+' SET excludedReason = \"Outgroup missing\" WHERE id == ?', (g,))\n",
    "            somethingMissing += 1\n",
    "    print('Number missing an outgroup species:',somethingMissing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOutgroupDup(table):\n",
    "    try:\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN notes TEXT')\n",
    "    except:\n",
    "        pass #col exists\n",
    "    cursor.execute('SELECT id, prunedTree FROM '+table+' WHERE excludedReason IS NULL')\n",
    "    geneTreePrunedDict = dict(cursor.fetchall())\n",
    "    singTreeOneEach, singTreeMulti = [], []\n",
    "    cursor.execute\n",
    "    for g in geneTreePrunedDict:\n",
    "        tree = PhyloTree(geneTreePrunedDict[g])\n",
    "        eug, sup, bia,suz = 0,0,0,0\n",
    "        for node in tree.traverse():\n",
    "            if 'DEUG' in node.name:\n",
    "                eug += 1\n",
    "            if 'DSUP' in node.name:\n",
    "                sup += 1\n",
    "            if 'DSUZ' in node.name:\n",
    "                suz += 1\n",
    "            if 'DBIA' in node.name:\n",
    "                bia += 1\n",
    "        if eug == 1 and suz == 1 and sup == 1 and bia == 1:\n",
    "            singTreeOneEach.append(g)\n",
    "        else:\n",
    "            cursor.execute('UPDATE singleton_trees SET notes = \"Multiple copies in outgroup\" WHERE id == ?', (g,))\n",
    "            singTreeMulti.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(List):\n",
    "    outList = []\n",
    "    for subList in List:\n",
    "        outList.extend(subList)\n",
    "    return(outList)\n",
    "\n",
    "def checkSplits(table):\n",
    "    #tree topology checks for splitting into multiple trees in cases where multicopy in the outgroup species\n",
    "    #I can check the 'check monophyly' method here when I have to update the species\n",
    "    #assign topology ids to all trees, check for straighforward duplications\n",
    "    outSp = ['DEUG','DBIA','DSUP','DSUZ']\n",
    "    try:\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN tree_top_id INTEGER')\n",
    "    except:\n",
    "        pass\n",
    "    cursor.execute('SELECT id, prunedTree FROM '+table+' WHERE notes == \"Multiple copies in outgroup\"')\n",
    "    geneTreePrunedDict = dict(cursor.fetchall())\n",
    "    top_dict = {}\n",
    "    print('Doing auto checks...',table)\n",
    "    un = 0\n",
    "    for g in geneTreePrunedDict:\n",
    "        unsuitable = False\n",
    "        tree = PhyloTree(geneTreePrunedDict[g])\n",
    "        for s in outSp: \n",
    "            #initial check, get all nodes for each outgroup species, check if they form a monophyletic group\n",
    "            #this should only be the case if it's a species specific dup, it's just to filter out some before the manual check\n",
    "            if unsuitable:\n",
    "                break\n",
    "            nodeList = []\n",
    "            for node in tree.traverse():\n",
    "                if s in node.name:\n",
    "                    nodeList.append(node.name)\n",
    "\n",
    "            if len(nodeList) > 1:\n",
    "                if tree.check_monophyly(values=nodeList,target_attr='name')[0]:\n",
    "                    #singTreeMulti.pop(g) #it might get upset about altering an object I'm iterating over, might need to keep a list\n",
    "                    cursor.execute('UPDATE '+table+' SET split = \"F\" WHERE id == ?',(g,))\n",
    "                    cursor.execute('UPDATE '+table+' SET excludedReason = \"Multiple copies in outgroup-split not possible/subtrees unsuitable\" WHERE id == ?',(g,))\n",
    "                    unsuitable = True\n",
    "        if unsuitable:\n",
    "            un += 1\n",
    "            continue\n",
    "        #set topology ids for any tree passing this point\n",
    "        top = tree.get_topology_id(attr='species')\n",
    "        try:\n",
    "            top_dict[top].append(g)\n",
    "        except KeyError:\n",
    "            top_dict[top] = [g]\n",
    "    print(un, 'trees unsuitable in auto checks')  \n",
    "    #for each topology, look at one tree and check if it's suitable to split\n",
    "    print('Assigning top IDs...')\n",
    "    for top in top_dict:\n",
    "        for g in top_dict[top]:\n",
    "            cursor.execute('UPDATE '+table+' SET tree_top_id = ? WHERE id == ?',(top,g))\n",
    "\n",
    "    #determine if trees can be split or not, manual inspection\n",
    "    print('Manual checks...')\n",
    "    manualDict = {}\n",
    "    for top in top_dict:\n",
    "        print(PhyloTree(geneTreePrunedDict[top_dict[top][0]]))\n",
    "        opinion = input('Opinion? Answer Y for splittable, N for not ')\n",
    "        while opinion != 'Y' and opinion != 'N':\n",
    "            opinion = input('Opinion? ')\n",
    "        manualDict[top] = opinion #must be either Y or N\n",
    "    \n",
    "    print('Setting outcome...')\n",
    "    #set excluded reason for trees determined to be disappointments\n",
    "    exTopList = [x for x in manualDict if manualDict[x] == 'N']\n",
    "    exSingMultiTreesList = [top_dict[x] for x in top_dict if x in exTopList]\n",
    "    exclGenes = flatten(exSingMultiTreesList)\n",
    "    for g in exclGenes:\n",
    "        cursor.execute('UPDATE '+table+' SET split = \"F\" WHERE id == ?',(g,))\n",
    "        cursor.execute('UPDATE '+table+' SET excludedReason = \"Multiple copies in outgroup-split not possible/subtrees unsuitable\" WHERE id == ?',(g,))\n",
    "\n",
    "    #update for trees that can be split\n",
    "    splitTopList = [x for x in manualDict if manualDict[x] == 'Y']\n",
    "    splitSingMultiTreesList = [top_dict[x] for x in top_dict if x in splitTopList]\n",
    "    splitTreeGenes = flatten(splitSingMultiTreesList)\n",
    "    for g in splitTreeGenes:\n",
    "        cursor.execute('UPDATE '+table+' SET split = \"T\" WHERE id == ?',(g,))\n",
    "    db.commit()\n",
    "    print('Done',table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86422afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_node_search(node,name):\n",
    "    if name in node.name:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def doSplits(table,outTable):\n",
    "    #split any trees that can be split, write the final singleton trees to new table (does this every 10 and at the end)\n",
    "    #add all the trees that are straightforward, single copy outgroup situations to the dict for inserting into table\n",
    "    cursor.execute('CREATE TABLE IF NOT EXISTS '+outTable+'(id TEXT, tree BLOB, excludedReason TEXT)')\n",
    "    cursor.execute('SELECT id, prunedTree FROM '+table+' WHERE excludedReason IS NULL AND (split != \"T\" OR split IS NULL)')\n",
    "    finalTreeDict = dict(cursor.fetchall())\n",
    "    exSingMultiTreesList2 = []\n",
    "    finalTrees = []\n",
    "    redoList = []\n",
    "    nodeNonsense = []\n",
    "    # cursor.execute('SELECT id, prunedTree FROM singleton_trees WHERE notes == \"Multiple copies in outgroup\"')\n",
    "    # geneTreePrunedDict = dict(cursor.fetchall())\n",
    "    count = 0\n",
    "    # tree splitting\n",
    "    cursor.execute('SELECT id FROM '+table+' WHERE split == \"T\"')\n",
    "    s = cursor.fetchall()\n",
    "    if s == []:\n",
    "        splitSingMultiTreesList = []\n",
    "        print('No trees to split, inserting suitable trees...',len([x for x in finalTreeDict.keys()]))\n",
    "        for g in finalTreeDict:\n",
    "            cursor.execute('INSERT INTO '+outTable+'(id,tree) VALUES (?,?)', (g,finalTreeDict[g]))\n",
    "            db.commit()\n",
    "        print('Done')\n",
    "    else:\n",
    "        splitSingMultiTreesList = [x[0] for x in s]\n",
    "    cursor.execute('SELECT id, prunedTree FROM '+table)\n",
    "    geneTreePrunedDict = dict(cursor.fetchall())\n",
    "    for g in splitSingMultiTreesList:\n",
    "        count += 1\n",
    "        error = False\n",
    "        currentTreeSplits = []\n",
    "        currentIndex = splitSingMultiTreesList.index(g)\n",
    "        tree = PhyloTree(geneTreePrunedDict[g])\n",
    "        tree.show()\n",
    "        multi = input('Multiple usable trees? Answer Yes or No (Or anything else if the tree is not suitable)')\n",
    "        if multi == 'No':\n",
    "            try:\n",
    "                node1Name = input(\"Node 1? Answer Forgot if didn't check the node name\")\n",
    "                node2Name = input('Node 2? ')\n",
    "                if node1Name == 'Forgot':\n",
    "                    tree.show()\n",
    "                    node1Name = input('Node 1? ')\n",
    "                    node2Name = input('Node 2? ')\n",
    "                node1 = [x for x in filter(lambda node: fancy_node_search(node, node1Name.strip()),tree.traverse())][0]\n",
    "                node2 = [x for x in filter(lambda node: fancy_node_search(node, node2Name.strip()),tree.traverse())][0]\n",
    "                #     get common ancester to detach at\n",
    "                splitTreeNode = node1.get_common_ancestor(node2)\n",
    "                # pretty sure detach will give the tree below that node\n",
    "                splitTree = splitTreeNode.detach()\n",
    "                splitTree.show()\n",
    "                okay = input('Okay?')\n",
    "                if okay == 'Y':\n",
    "                    currentTreeSplits.append(splitTree)\n",
    "                elif okay == 'N':\n",
    "                    break\n",
    "                else:\n",
    "                    check = input('All good?')\n",
    "            except AttributeError:\n",
    "                    nodeNonsense.append(g, geneTreePrunedDict[g])\n",
    "                    continue\n",
    "                    error = True\n",
    "        elif multi == 'Yes':\n",
    "            done = 'Y'\n",
    "            while done == 'Y':\n",
    "                try:\n",
    "                    tree.show()\n",
    "                    node1Name = input('Node 1? ')\n",
    "                    node2Name = input('Node 2? ')\n",
    "                    node1 = [x for x in filter(lambda node: fancy_node_search(node, node1Name.strip()),tree.traverse())][0]\n",
    "                    node2 = [x for x in filter(lambda node: fancy_node_search(node, node2Name.strip()),tree.traverse())][0]\n",
    "                    #     get common ancester to detach at\n",
    "                    splitTreeNode = node1.get_common_ancestor(node2)\n",
    "                    # pretty sure detach will give the tree below that node\n",
    "                    splitTree = splitTreeNode.detach()\n",
    "                    splitTree.show()\n",
    "                    okay = input('Is the tree okay? Answer Y or N')\n",
    "                    if okay == 'Y':\n",
    "                        currentTreeSplits.append(splitTree)\n",
    "                    elif okay == 'N':\n",
    "                        break\n",
    "                    else:\n",
    "                        check = input('All good?')\n",
    "                        if check == 'Y':\n",
    "                            currentTreeSplits.append(splitTree)\n",
    "                        elif check == 'N':\n",
    "                            break\n",
    "\n",
    "                    done = input('Any more trees? Answer Y or N')\n",
    "                except AttributeError:\n",
    "                    print('Something went wrong with the tree: unfinished!!')\n",
    "                    nodeNonsense.append((g, geneTreePrunedDict[g]))\n",
    "                    done = 'N'\n",
    "                    error = True\n",
    "                except:\n",
    "                    done = 'Y'\n",
    "                    print('Try again!')\n",
    "\n",
    "\n",
    "        else:\n",
    "            check = input('Did you mean to do that? Answer Yes or No')\n",
    "            if check == 'No':\n",
    "                redoList.append((g,geneTreePrunedDict[g]))\n",
    "                error = True\n",
    "            elif check == 'Yes':\n",
    "                exSingMultiTreesList2.append(g)\n",
    "                \n",
    "        if not error:\n",
    "            finalTrees.extend(currentTreeSplits)\n",
    "        else:\n",
    "            print('Something went wrong somewhere for gene '+g+'!')\n",
    "            print('Lists for accidents and node issues will be returned')\n",
    "        \n",
    "        if count%10 == 0 or count == len(splitSingMultiTreesList):\n",
    "            print(\"You've done \" + str(count) + ', good job!')\n",
    "            for tree in finalTrees:\n",
    "                for node in tree.traverse():\n",
    "                    if 'Dere' in node.name:\n",
    "                        geneName = re.search('.*_(FB.*)_',node.name).group(1)\n",
    "                        finalTreeDict[geneName] = tree.write()\n",
    "\n",
    "            for g in finalTreeDict:\n",
    "                print('Running insert...')\n",
    "                cursor.execute('INSERT INTO '+outTable+'(id,tree) VALUES (?,?)', (g,finalTreeDict[g]))\n",
    "            db.commit()\n",
    "            print('Done')\n",
    "            finalTreeDict = {}\n",
    "            finalTrees = []\n",
    "\n",
    "    # remove all the trees that turned out to be no use on a second look\n",
    "    for g in exSingMultiTreesList2:\n",
    "        cursor.execute('UPDATE '+table+' SET excludedReason = \"Multiple copies in outgroup-split not possible/subtrees unsuitable\" WHERE id == ?',(g,))\n",
    "    db.commit()\n",
    "    if redoList != [] or nodeNonsense != []:\n",
    "        return [redoList,nodeNonsense]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb186119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignDupStatus(table):\n",
    "    #assign duplication status\n",
    "    try:\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COlUMN dup_status TEXT')\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN dupInSp TEXT')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    cursor.execute('SELECT id, tree FROM '+table)\n",
    "    finalTreeDict = dict(cursor.fetchall())\n",
    "\n",
    "    i = 0\n",
    "    dupc, presc = 0,0\n",
    "    both = 0\n",
    "    for g in finalTreeDict:\n",
    "        dup, pres = False, False\n",
    "        tree = PhyloTree(finalTreeDict[g])\n",
    "        for n in tree.traverse():\n",
    "            n.set_species_naming_function(get_species)\n",
    "        sim, ere, mel, yak, sec, mau, san = 0,0,0,0,0 #counts for each species\n",
    "\n",
    "        for node in tree.traverse():\n",
    "\n",
    "            if node.name == '' or node.name.is_numeric():\n",
    "                continue #internal nodes, no label or bootstrap value\n",
    "\n",
    "            if 'DSIM' in node.name:\n",
    "                sim += 1\n",
    "            elif 'DERE' in node.name:\n",
    "                ere += 1\n",
    "            elif 'DMEL' in node.name:\n",
    "                mel += 1\n",
    "            elif 'DSEC' in node.name:\n",
    "                sec += 1\n",
    "            elif 'DYAK' in node.name:\n",
    "                yak += 1\n",
    "            elif 'DMAU' in node.name:\n",
    "                mau += 1\n",
    "            elif 'DSAN' in node.name:\n",
    "                san += 1\n",
    "            dupStatusList = [sim, ere, mel, yak, sec, mau, san]\n",
    "        dup_test = map(lambda x: x <=1, dupStatusList)\n",
    "        presence_test = map(lambda x: x > 0, dupStatusList)\n",
    "    #     all returns False if any elements are False\n",
    "        if not all(dup_test): #there is a duplicated gene somewhere, x<= 1 returned false at some point\n",
    "            cursor.execute('UPDATE '+table+' SET dup_status = \"D\" WHERE id == ?', (g,))\n",
    "            dupList = []\n",
    "            spDict = {'Dyak': yak, 'Dsec':sec, 'Dmel':mel, 'Dsim':sim, 'Dere':ere,'Dmau':mau,'Dsan':san}\n",
    "            for s in spDict:\n",
    "                if spDict[s] > 1:\n",
    "                    dupList.append(s)\n",
    "    #                 print(dupList)\n",
    "            cursor.execute('UPDATE '+table+' SET dupInSp = ? WHERE id == ?',(','.join(dupList),g))\n",
    "            dup = True\n",
    "        elif all(presence_test):\n",
    "            cursor.execute('UPDATE '+table+' SET dup_status = \"S\" WHERE id == ?', (g,))\n",
    "            pres = True\n",
    "            presc += 1\n",
    "        else: #exclude, not duplicated and some species missing so can't guarantee either status\n",
    "            cursor.execute('UPDATE '+table+' SET excludedReason = \"Missing species: no duplications\" WHERE id == ?', (g,))\n",
    "            both += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDupTiming(table):\n",
    "    # checking paralogs aren't ancestral duplications \n",
    "    maybeProblemCount = 0\n",
    "    cursor.execute('SELECT id, tree FROM '+table+' WHERE dup_status == \"D\"')\n",
    "    dupTreeDict = dict(cursor.fetchall())\n",
    "    i =0\n",
    "    okCount = 0\n",
    "    for g in dupTreeDict:\n",
    "        tree = PhyloTree(dupTreeDict[g])\n",
    "        dupSp = []\n",
    "        outNode = None\n",
    "        dupStatusList = []\n",
    "        ok = True\n",
    "        for node in tree.traverse():\n",
    "            if node.name == '':\n",
    "                continue #internal nodes\n",
    "\n",
    "            if 'DSIM' in node.name:\n",
    "                dupStatusList.append('DSIM')\n",
    "            elif 'DERE' in node.name:\n",
    "                dupStatusList.append('DERE')\n",
    "            elif 'DMEL' in node.name:\n",
    "                dupStatusList.append('DMEL')\n",
    "            elif 'DYAK' in node.name:\n",
    "                dupStatusList.append('DYAK')\n",
    "            elif 'DSEC' in node.name:\n",
    "                dupStatusList.append('DSEC')\n",
    "            elif 'DMAU' in node.name:\n",
    "                dupStatusList.append('DMAU')\n",
    "            elif 'DSAN' in node.name:\n",
    "                dupStatusList.append('DSAN')\n",
    "        for sp in set(dupStatusList):\n",
    "            if dupStatusList.count(sp) > 1:\n",
    "                dupSp.append(sp)\n",
    "\n",
    "        for sp in dupSp:\n",
    "            inNodeList = []\n",
    "            for node in tree.traverse():\n",
    "                if node.name == '':\n",
    "                    continue\n",
    "                if sp in node.name:\n",
    "                    inNodeList.append(node)\n",
    "\n",
    "                if 'DSUZ' in node.name:\n",
    "                    outNode = node\n",
    "            if len(inNodeList) > 2: #need to do selection of which pair to take\n",
    "                min_dist = None\n",
    "                for a,b in itertools.combinations(inNodeList,2):\n",
    "                    dist = tree.get_distance(a,b)\n",
    "                    if min_dist == None:\n",
    "                        min_dist = dist\n",
    "                        min_pair = (a,b)\n",
    "                    elif dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        min_pair = (a,b)\n",
    "                inNodeList = [min_pair[0],min_pair[1]]\n",
    "\n",
    "        #     otherwise:\n",
    "            betweenParalogs = tree.get_distance(inNodeList[0],inNodeList[1])\n",
    "            outDistance1 = tree.get_distance(inNodeList[0],outNode)\n",
    "            outDistance2 = tree.get_distance(inNodeList[1],outNode)\n",
    "\n",
    "            if (betweenParalogs < outDistance1) and (betweenParalogs < outDistance2):\n",
    "                ok = True\n",
    "            else:\n",
    "                ok = False\n",
    "\n",
    "            if ok == False:\n",
    "                cursor.execute('UPDATE '+table+' SET excludedReason = \"Possible ancestral duplication\" WHERE id == ?',(g,))\n",
    "                break\n",
    "        else:\n",
    "            okCount +=1\n",
    "\n",
    "    print('This many trees pass to this point: ',okCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec09a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkCorrectOutgroups(table):\n",
    "    #  check outgroups actually are outgroups by checking monophyly of the other species\n",
    "    cursor.execute('SELECT id, tree FROM '+table+' WHERE (NOT (dup_status IS NULL)) AND (excludedReason IS NULL)')\n",
    "    treeDict = dict(cursor.fetchall())\n",
    "    for g in treeDict:\n",
    "        spList = ['DMEL','DSEC','DSIM','DYAK','DERE','DSAN','DMAU']\n",
    "        tree = PhyloTree(treeDict[g], sp_naming_function=get_species)\n",
    "        try:\n",
    "            check = tree.check_monophyly(values=spList,target_attr='species')\n",
    "\n",
    "        except ValueError: # in duplicable trees, there may be species missing\n",
    "            spList2 = []\n",
    "            for s in spList:\n",
    "                 if s in [node.species for node in tree.traverse()]:\n",
    "                        spList2.append(s)\n",
    "            check = tree.check_monophyly(values=spList2,target_attr='species')\n",
    "        if check[0] == False:\n",
    "            cursor.execute('UPDATE '+table+' SET excludedReason = \"Incorrect outgroups\" WHERE id == ?',(g,))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36907272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_prot_id(node): #from trees\n",
    "    import re\n",
    "    name = node.name\n",
    "    split = name.split(\"_\")\n",
    "    if len(split) == 3: #no internal underscores\n",
    "        prot = split[1]\n",
    "    elif len(split) == 4:\n",
    "        prot = '_'.join(split[1:3])\n",
    "    elif len(split) == 5:\n",
    "        prot = '_'.join(split[2:4])\n",
    "    else:\n",
    "        prot = '_'.join(split[-3:-1])\n",
    "    return prot\n",
    "def retrieve_prot_id_align(string):\n",
    "    prot = string.split('|')[1]\n",
    "    return prot\n",
    "def retrieve_species(node):\n",
    "    import re\n",
    "    if type(node) == str:\n",
    "        sp = node[-4:]\n",
    "    else:\n",
    "        sp = node.name[-4:]\n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rateCalc(table,alignmentDir,origTable):\n",
    "    #rate calc and comparison\n",
    "    headerEndList= ['DMEL','DSIM','DSEC','DYAK','DERE','DSAN','DSUP','DSUZ','DBIA','DMAU','DEUG']\n",
    "    try:\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN confirm_rate REAL')\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN proxy_rate REAL')\n",
    "\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN confirm_dS REAL')\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN proxy_dS REAL')\n",
    "\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN confirm_dN REAL')\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN proxy_dN REAL')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # select trees\n",
    "    cursor.execute('SELECT id,tree FROM '+table+' WHERE excludedReason IS NULL')\n",
    "    trees = cursor.fetchall()\n",
    "\n",
    "    for base_id, tree in tqdm(trees, miniters=50, mininterval=60):\n",
    "        cursor.execute('SELECT realGroup FROM '+ origTable+' WHERE id == ?',(base_id,))\n",
    "        realGroup = cursor.fetchall()[0][0]\n",
    "        c = False\n",
    "        cdsDict = {}\n",
    "        musInput = ''\n",
    "        alignList = []\n",
    "        DmelCount = 0\n",
    "\n",
    "        tree2 = PhyloTree(tree)\n",
    "    #     get the number of D. mel paralogs\n",
    "        for node in tree2.traverse():\n",
    "            if 'DMEL' in node.name:\n",
    "                DmelCount += 1\n",
    "        if DmelCount == 0:\n",
    "            cursor.execute('UPDATE '+table+' SET excludedReason = \"Missing Dmel\" WHERE id == ?',(base_id,))\n",
    "            print('No DMEL in tree')\n",
    "            continue\n",
    "    #     need to get the correct sequences from files - have muscle alignments already from tree building\n",
    "#         get the cds seq for each gene in tree, store in dict\n",
    "        for node in tree2.traverse():\n",
    "            if node.name == '' or node.name.isnumeric():\n",
    "                continue\n",
    "            ID = retrieve_prot_id(node)\n",
    "            sp = retrieve_species(node)\n",
    "\n",
    "            cmdCDS = ['sed', '-n', '-e', '/'+ ID +'/,/>/ p', 'dipteraCDS_raw/'+sp+'_raw_cds.fna']\n",
    "            pCDS = Popen(cmdCDS, stdout=PIPE,stderr=PIPE)\n",
    "            out,err = pCDS.communicate()\n",
    "            \n",
    "            CDSseq = out.split(bytes('\\n','utf-8'))[1:-2]\n",
    "            CDSseq = ''.join([x.decode('utf-8') for x in CDSseq])\n",
    "\n",
    "            cdsDict['>'+ID + ':' + sp] = CDSseq\n",
    "        if str(realGroup).startswith('OMA'):\n",
    "            alignFile = alignmentDir + '/' + str(realGroup) + '_alignment.fa'\n",
    "        else:\n",
    "            alignFile = alignmentDir + '/group' + str(realGroup) + '_alignment.fa'\n",
    "        #TODO: what exactly is going on with this alignment file, am I putting *all* the sequences into interProt.fa??\n",
    "        with open(alignFile,'r') as prot_file, open('interCDS.fa','w') as cds_file, open('interProt.fa','w') as prot_align_file:\n",
    "    #         muscle outputs alignments in a different order to input sequences\n",
    "    #         cds sequences have to be written in same order to work with pal2nal\n",
    "            alignSeq = ''\n",
    "            alignDict = {}\n",
    "            #put entire protein alignment in a string\n",
    "            for line in prot_file:\n",
    "                alignSeq = alignSeq + line\n",
    "            \n",
    "            #get the order the headers occur in\n",
    "            order= [line for line in alignSeq.split('\\n') if line.startswith('>')]\n",
    "            #make new headers based on this\n",
    "#             order2 = ['>'+retrieve_prot_id_align(x)+':'+retrieve_species(x) for x in order]\n",
    "            order2=order\n",
    "#             create new protein alignment file with \n",
    "            incl = False #changed from True, shouldn't really matter because it'll hit a header first\n",
    "            for line in alignSeq.split('\\n'):\n",
    "                if line.startswith('>'):\n",
    "#                     newHead = '>'+retrieve_prot_id_align(line)+':'+retrieve_species(line)\n",
    "                    newHead = line\n",
    "                    if newHead in order2:\n",
    "                        alignDict[newHead] = ''\n",
    "                        current = newHead\n",
    "                        incl = True\n",
    "                    else:\n",
    "                        incl = False\n",
    "                elif incl == True:\n",
    "                    alignDict[current] = alignDict[current] + line\n",
    "            # get CDS in right order as well, write each header and sequence to CDS/prot file\n",
    "            for x in order2:\n",
    "                try:\n",
    "                    i = [y for y in cdsDict if retrieve_prot_id_align(x) in y][0]\n",
    "                    seq = cdsDict[i]\n",
    "                except IndexError:\n",
    "                    continue #species not included in pruned trees ...I guess this is where I'm filtering??? -Looks that way\n",
    "                cds_file.write(x.replace('|','_') + '\\n')\n",
    "                cds_file.write(seq + '\\n')\n",
    "                prot_align_file.write(x.replace('|','_') + '\\n')\n",
    "                prot_align_file.write(alignDict[x] + '\\n')    \n",
    "        # somehow convert to codon alignments - pal2nal\n",
    "        #     usage: pal2nal prot_alignment dna_seq -output paml\n",
    "    #     print('at pal2nal')\n",
    "        cm = '../SOFTWARE/pal2nal.pl interProt.fa interCDS.fa -output paml'\n",
    "        p2 = Popen(cm.split(' '), stdout=PIPE, stderr=PIPE)\n",
    "        out2, err2 = p2.communicate()\n",
    "\n",
    "        with open('interCodonAlign.paml','w') as cod_file, open('example.tree','w') as treeFile:\n",
    "            cod_file.write(out2.decode())\n",
    "            treeFile.write(tree)\n",
    "    #     print('Finished that, on to sequence extracting')\n",
    "    # #     extract specific sequences for pairwise dn/ds calc\n",
    "    # #     need to take the first line for paml format to work, also need to change the first number to no. of seqs\n",
    "        with open('interCodonAlign.paml','r') as inFile:\n",
    "            for line in inFile:\n",
    "                alignList.append(line)   \n",
    "        \n",
    "        with open('paml_input1.paml','w') as outFile, open('paml_input2.paml','w') as outFile2:\n",
    "            dmelFound = False\n",
    "            for i,line in enumerate(alignList):\n",
    "                if i == 0:\n",
    "                    testLine = line.split(' ')\n",
    "                    intCount = 0\n",
    "                    for ob in testLine:\n",
    "                        if ob == '':\n",
    "                            pass\n",
    "                        else:\n",
    "                            intCount += 1\n",
    "                            if intCount == 2:\n",
    "                                seqLen = ob\n",
    "                    outLine = '  ' + '2' + '   ' + seqLen + '\\n'\n",
    "    #                 # number of sequences in the dmel file might be greater than 2 due to dups, = dyak paralogs +1\n",
    "                    outLine2 = '  ' + str(DmelCount + 1) + '   ' + seqLen + '\\n'\n",
    "                    outFile.write(outLine) #proxy rate\n",
    "                    outFile2.write(outLine2) #confirm rate\n",
    "                else:\n",
    "                    if 'DEUG' in line:\n",
    "                        outFile.write(line)\n",
    "                        for line2 in alignList[i+1:]:\n",
    "#                             if not line2.startswith('F') and not line2.startswith('X') and not line2.startswith('N'):\n",
    "                            if line2.strip('\\n')[-4:] not in headerEndList:\n",
    "                                outFile.write(line2)\n",
    "                            else:\n",
    "                                break\n",
    "                        outFile2.write(line)\n",
    "                        for line2 in alignList[i+1:]: #write all lines following the 'DEUG' line until next id line\n",
    "#                             if not line2.startswith('F') and not line2.startswith('X') and not line2.startswith('N'):\n",
    "                            if line2.strip('\\n')[-4:] not in headerEndList:\n",
    "                                outFile2.write(line2)\n",
    "                            else:\n",
    "                                break\n",
    "                    elif 'DMEL' in line:\n",
    "                        dmelFound = True\n",
    "                        outFile2.write(line)\n",
    "                        for line2 in alignList[i+1:]:\n",
    "#                             if not line2.startswith('F') and not line2.startswith('X') and not line2.startswith('N'):\n",
    "                            if line2.strip('\\n')[-4:] not in headerEndList:\n",
    "                                outFile2.write(line2)\n",
    "                            else:\n",
    "                                break\n",
    "                    elif 'DSUZ' in line:\n",
    "                        outFile.write(line)\n",
    "                        for line2 in alignList[i+1:]:\n",
    "#                             if not line2.startswith('F') and not line2.startswith('X') and not line2.startswith('N'):\n",
    "                            if line2.strip('\\n')[-4:] not in headerEndList:\n",
    "                                outFile.write(line2)\n",
    "                            else:\n",
    "                                break\n",
    "#         break\n",
    "    #     print('Sequences extracted')\n",
    "        if not dmelFound:\n",
    "            cursor.execute('UPDATE '+table+' SET excludedReason = \"missing Dmel ortholog\" WHERE id == ?',(base_id,))\n",
    "            print('Somehow I passed the first check but without Dmel') #likely \n",
    "            continue\n",
    "    #     print('Running PAML now')\n",
    "    #     # run PAML on alignments - this needs worked on, need to get the mean rate across Dmel duplicates\n",
    "        cmd = codeml.Codeml(alignment='paml_input1.paml', tree='example.tree', out_file='results.out',working_dir='.')\n",
    "        cmd.read_ctl_file('codeml.ctl')\n",
    "        cmd.alignment='paml_input1.paml'\n",
    "        cmd.verbose = True\n",
    "        try:\n",
    "            output = cmd.run(command='../SOFTWARE/paml4.9j/bin/codeml')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    #         print('abandon all hope, we have a PAML issue')\n",
    "            continue\n",
    "        \n",
    "    #    extract dn/ds value for each pair\n",
    "        id1 = [x for x in output['pairwise'].keys()][0]\n",
    "        id2 = [x for x in output['pairwise'].keys()][1]\n",
    "\n",
    "        resDict = output['pairwise'][id1]\n",
    "        ds3 = resDict[id2]['dS']\n",
    "        dn3 = resDict[id2]['dN']\n",
    "        dnds3 = resDict[id2]['omega']\n",
    "\n",
    "        cursor.execute('UPDATE '+table+' SET proxy_rate = ? WHERE id == ?',(dnds3,base_id))\n",
    "        cursor.execute('UPDATE '+table+' SET proxy_dS = ? WHERE id == ?',(ds3,base_id))\n",
    "        cursor.execute('UPDATE '+table+' SET proxy_dN = ? WHERE id == ?',(dn3,base_id))\n",
    "\n",
    "    #      run PAML on alignment2 (confirm)\n",
    "        cmd = codeml.Codeml(alignment='paml_input2.paml', tree='example.tree', out_file='results.out',working_dir='.')\n",
    "        cmd.read_ctl_file('codeml.ctl')\n",
    "        cmd.alignment='paml_input2.paml'\n",
    "        try:\n",
    "            output = cmd.run(command='../SOFTWARE/paml4.9j/bin/codeml') #might need to add parse = True\n",
    "        except Exception as e:\n",
    "            print('File 2')\n",
    "            print(e)\n",
    "    #         print('abandon all hope, we have a PAML issue')\n",
    "            continue\n",
    "\n",
    "        id1 = [x for x in output['pairwise'].keys() if 'DEUG' in x][0]\n",
    "        id2List = [x for x in output['pairwise'].keys()]\n",
    "        dndsList = []\n",
    "        dsList, dnList = [],[]\n",
    "\n",
    "        resDict = output['pairwise'][id1]\n",
    "        for id2 in [x for x in id2List if x != id1]:\n",
    "            ds = resDict[id2]['dS']\n",
    "            dn = resDict[id2]['dN']\n",
    "            dnds = resDict[id2]['omega']\n",
    "\n",
    "            dndsList.append(dnds)\n",
    "            dsList.append(ds)\n",
    "            dnList.append(dn)\n",
    "\n",
    "\n",
    "            dnds2 = np.mean(dndsList)\n",
    "            dn2 = np.mean(dnList)\n",
    "            ds2 = np.mean(dsList)\n",
    "\n",
    "    #     print('all done the second alignment,',ds2,dn2,dnds2)  \n",
    "        cursor.execute('UPDATE '+table+' SET confirm_rate = ? WHERE id == ?',(dnds2,base_id))\n",
    "        cursor.execute('UPDATE '+table+' SET confirm_dS = ? WHERE id == ?',(ds2,base_id))\n",
    "        cursor.execute('UPDATE '+table+' SET confirm_dN = ? WHERE id == ?',(dn2,base_id))\n",
    "\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rateComp(table,pairing,limitDS=False):\n",
    "    if pairing == 'confirm' and not limitDS:\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        print('Confirm rate p val:')\n",
    "        print(mannwhitneyu(singList_rate, dupList_rate,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_rate),np.median(dupList_rate))\n",
    "        print('Means:',np.mean(singList_rate),np.mean(dupList_rate))\n",
    "        print('')\n",
    "\n",
    "        print('Confirm dN p val:')\n",
    "        print(mannwhitneyu(singList_dn, dupList_dn,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_dn),np.median(dupList_dn))\n",
    "        print('Means:',np.mean(singList_dn),np.mean(dupList_dn))\n",
    "\n",
    "        print('Confirm dS p val:')\n",
    "        print(mannwhitneyu(singList_ds, dupList_ds,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_ds),np.median(dupList_ds))\n",
    "        print('Means:',np.mean(singList_ds),np.mean(dupList_ds))\n",
    "        print('')\n",
    "    \n",
    "    elif pairing == 'confirm':\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        print('Confirm rate p val:')\n",
    "        print(mannwhitneyu(singList_rate, dupList_rate,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_rate),np.median(dupList_rate))\n",
    "        print('Means:',np.mean(singList_rate),np.mean(dupList_rate))\n",
    "        print('')\n",
    "\n",
    "        print('Confirm dN p val:')\n",
    "        print(mannwhitneyu(singList_dn, dupList_dn,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_dn),np.median(dupList_dn))\n",
    "        print('Means:',np.mean(singList_dn),np.mean(dupList_dn))\n",
    "\n",
    "        print('Confirm dS p val:')\n",
    "        print(mannwhitneyu(singList_ds, dupList_ds,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_ds),np.median(dupList_ds))\n",
    "        print('Means:',np.mean(singList_ds),np.mean(dupList_ds))\n",
    "        print('')\n",
    "    \n",
    "    elif pairing == 'proxy' and not limitDS:\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        print('Proxy rate p val:')\n",
    "        print(mannwhitneyu(singList_rate, dupList_rate,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_rate),np.median(dupList_rate))\n",
    "        print('Means:',np.mean(singList_rate),np.mean(dupList_rate))\n",
    "        print('')\n",
    "\n",
    "        print('Proxy dN p val:')\n",
    "        print(mannwhitneyu(singList_dn, dupList_dn,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_dn),np.median(dupList_dn))\n",
    "        print('Means:',np.mean(singList_dn),np.mean(dupList_dn))\n",
    "\n",
    "        print('Proxy dS p val:')\n",
    "        print(mannwhitneyu(singList_ds, dupList_ds,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_ds),np.median(dupList_ds))\n",
    "        print('Means:',np.mean(singList_ds),np.mean(dupList_ds))\n",
    "        print('')\n",
    "    \n",
    "    elif pairing == 'proxy':\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        print('Proxy rate p val:')\n",
    "        print(mannwhitneyu(singList_rate, dupList_rate,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_rate),np.median(dupList_rate))\n",
    "        print('Means:',np.mean(singList_rate),np.mean(dupList_rate))\n",
    "        print('')\n",
    "\n",
    "        print('Proxy dN p val:')\n",
    "        print(mannwhitneyu(singList_dn, dupList_dn,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_dn),np.median(dupList_dn))\n",
    "        print('Means:',np.mean(singList_dn),np.mean(dupList_dn))\n",
    "\n",
    "        print('Proxy dS p val:')\n",
    "        print(mannwhitneyu(singList_ds, dupList_ds,alternative='two-sided'))\n",
    "        print('Medians:',np.median(singList_ds),np.median(dupList_ds))\n",
    "        print('Means:',np.mean(singList_ds),np.mean(dupList_ds))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchPval(list1,list2):\n",
    "    p = mannwhitneyu(list1,list2,alternative='two-sided').pvalue\n",
    "    outString = str(round(p,4))\n",
    "    return outString\n",
    "def generateRateCompFigure(table,pairing,limitDS=False): #CHECK - anything from here on might be altered on the mac version\n",
    "    if pairing == 'confirm' and not limitDS:\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "    \n",
    "    elif pairing == 'confirm':\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_rate FROM processed_trees WHERE NOT (confirm_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT confirm_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (confirm_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "    \n",
    "    elif pairing == 'proxy' and not limitDS:\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "    \n",
    "    elif pairing == 'proxy':\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"S\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_rate FROM processed_trees WHERE NOT (proxy_rate IS NULL) AND dup_status == \"D\" AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_rate = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dS FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dS IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_ds = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"S\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        singList_dn = [x[0] for x in cursor.fetchall()]\n",
    "\n",
    "        cursor.execute('SELECT proxy_dN FROM processed_trees WHERE dup_status == \"D\" AND NOT (proxy_dN IS NULL) AND excludedReason IS NULL AND confirm_dS <= 4')\n",
    "        dupList_dn = [x[0] for x in cursor.fetchall()]\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(1,3,figsize=(30,10))\n",
    "    ax1, ax2, ax3 = axes\n",
    "    sns.set()\n",
    "    \n",
    "    for a in [ax1,ax2,ax3]:\n",
    "        a.yaxis.set_tick_params(labelsize=15)\n",
    "        a.xaxis.set_tick_params(labelsize=15)\n",
    "    if pairing == 'confirm':\n",
    "        ax2.text(0.5,1.3,'Confirmatory comparisons: D. eugracilis v D. melanogaster',fontdict=dict(size=16))\n",
    "    elif pairing == 'proxy':\n",
    "        ax2.text(0.5,1.3,'Proxy comparisons: D. eugracilis v D. suzukii',fontdict=dict(size=16))\n",
    "    \n",
    "    b= ax1.boxplot([singList_dn,dupList_dn], showmeans= True, meanline= True, flierprops=dict(markersize=1),labels=['Singleton','Duplicate'],patch_artist=True)\n",
    "\n",
    "    b1['boxes'][0].set_fc('#DC3220')\n",
    "    b1['boxes'][1].set_fc('#005AB5')\n",
    "    for m in b1['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "\n",
    "    ax1.set_ylabel('dN',fontdict=dict(fontsize=15))\n",
    "    \n",
    "\n",
    "\n",
    "    b2= ax2.boxplot([singList_ds,dupList_ds], flierprops=dict(markersize=1),labels=['Singleton','Duplicate'],patch_artist=True)\n",
    "    \n",
    "    b2['boxes'][0].set_fc('#DC3220')\n",
    "    b2['boxes'][1].set_fc('#005AB5')\n",
    "    for m in b2['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "\n",
    "    ax2.set_ylabel('dS',fontdict=dict(fontsize=15))\n",
    "    \n",
    "\n",
    "\n",
    "    b3= ax3.boxplot([singList_rate,dupList_rate], flierprops=dict(markersize=1),labels=['Singleton','Duplicate'],patch_artist=True)\n",
    "\n",
    "    b3['boxes'][0].set_fc('#DC3220')\n",
    "    b3['boxes'][1].set_fc('#005AB5')\n",
    "    for m in b3['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "\n",
    "    ax3.set_ylabel('dN/dS',fontdict=dict(fontsize=15))\n",
    "    \n",
    "\n",
    "    ax1.text(0.24,0.01,'p = '+fetchPval(singList_dn,dupList_dn), fontsize=16, ha='center',transform=fig.transFigure) #need to change these? Might just put p vals on plot\n",
    "    ax2.text(0.515,0.01,'p = '+fetchPval(singList_ds,dupList_ds), fontsize=16, ha='center',transform=fig.transFigure)\n",
    "    ax3.text(0.79,0.01,'p = '+fetchPval(singList_rate,dupList_rate), fontsize=16, ha='center',transform=fig.transFigure)\n",
    "    \n",
    "    if pairing == 'confirm' and not limitDS:\n",
    "        plt.savefig('confirmComp_'+table+'_final.eps',bbox_inches='tight')\n",
    "    elif pairing == 'confirm':\n",
    "        plt.savefig('confirmComp_dsUnder4_'+table+'_final.eps',bbox_inches='tight')\n",
    "    elif pairing == 'proxy' and not limitDS:\n",
    "        plt.savefig('proxyComp_'+table+'_final.eps',bbox_inches='tight')\n",
    "    elif pairing == 'proxy':\n",
    "        plt.savefig('proxyComp_dsUnder4_'+table+'_final.eps',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b36253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confounders: pairwise comparisons for possible confounders and correlations with rate\n",
    "def gc_percent(seq):\n",
    "    length = len(seq)\n",
    "    gc = seq.count('G') + seq.count('C')\n",
    "    return gc/length\n",
    "def confounderInsert(table):\n",
    "    cursor.execute('SELECT id FROM ' + table +' WHERE excludedReason IS NULL')\n",
    "    res = [x[0] for x in cursor.fetchall()]\n",
    "    singList = [re.search('^(.*?)\\|',x).group(1) for x in res] #protein ids\n",
    "    idDict = dict(zip(singList,res))\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('ALTER TABLE ' + table +' ADD COLUMN cdsLen INTEGER')\n",
    "        cursor.execute('ALTER TABLE ' + table +' ADD COLUMN gc REAL')\n",
    "        cursor.execute('ALTER TABLE ' + table +' ADD COLUMN gc3 REAL')\n",
    "        cursor.execute('ALTER TABLE ' + table +' ADD COLUMN exp REAL')\n",
    "    except:\n",
    "        pass\n",
    "    res = [x[0] for x in cursor.fetchall()]\n",
    "    \n",
    "\n",
    "    seqDict = {}\n",
    "    incSeq = False\n",
    "    currentSeq = ''\n",
    "    currentID = None\n",
    "\n",
    "    #fetch cds sequence for each gene\n",
    "\n",
    "    with open('dipteraCDS_raw/DSUZ_raw_cds.fna','r') as file:\n",
    "        print('started')\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            if line.startswith('>') and re.search('\\[protein_id=(.*?)\\]',line).group(1) in singList:\n",
    "                if currentID:#two inc seqs in a row\n",
    "                    seqDict[currentID] = currentSeq\n",
    "                    currentID = None\n",
    "                    currentSeq = ''\n",
    "                    currentID = re.search('\\[protein_id=(.*?)\\]',line).group(1)\n",
    "                else:\n",
    "                    currentID = re.search('\\[protein_id=(.*?)\\]',line).group(1)\n",
    "                    incSeq = True\n",
    "            elif line.startswith('>'):\n",
    "                if currentID:\n",
    "                    seqDict[currentID] = currentSeq\n",
    "                    currentID = None\n",
    "                    currentSeq = ''\n",
    "                    incSeq = False\n",
    "            elif incSeq:\n",
    "                currentSeq = currentSeq + line\n",
    "\n",
    "    #length and gc content , check case of sequence, check id format in cds file and exp output\n",
    "    for gene in seqDict:\n",
    "        l = len(seqDict[gene])\n",
    "        gc = gc_percent(seqDict[gene])\n",
    "        pos3 = seqDict[gene][2::3]\n",
    "        gc3 = gc_percent(pos3)\n",
    "        cursor.execute('UPDATE ' + table + ' SET cdsLen = ?, gc = ?, gc3 = ? WHERE id == ?',(l,gc,gc3,idDict[gene]))\n",
    "    #expression\n",
    "    with open('dSuz_exp.genes.results','r') as file: \n",
    "        file.readline()\n",
    "        for line in file:\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            ID, tpm = re.search('gene-(.*)$',line[0]).group(1),float(line[5])\n",
    "            if ID in singList:\n",
    "                cursor.execute('UPDATE ' +table+ ' SET exp = ? WHERE id == ?',(tpm,idDict[ID]))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8cc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confounderCompCorr(table):\n",
    "    #comparison of features between sing and dup genes, correlations\n",
    "    cursor.execute('SELECT cdsLen, gc, gc3, exp FROM ' + table +' WHERE dup_status == \"S\" AND excludedReason IS NULL AND proxy_dS < 4')\n",
    "    sRes = cursor.fetchall()\n",
    "    cursor.execute('SELECT cdsLen, gc, gc3, exp FROM ' + table +' WHERE dup_status == \"D\" AND excludedReason IS NULL AND proxy_dS < 4')\n",
    "    dRes = cursor.fetchall()\n",
    "    cursor.execute('SELECT cdsLen, gc, gc3, exp, proxy_rate FROM ' + table +' WHERE excludedReason IS NULL AND proxy_rate IS NOT NULL AND proxy_dS < 4')\n",
    "    totRes = cursor.fetchall()\n",
    "    print(len(sRes),len(dRes))\n",
    "    print('CDS Length:') \n",
    "    print('Sing vs Dup comp:')\n",
    "    print(mannwhitneyu([log10(x[0]) if x[0] > 0 else 0 for x in sRes],[log10(x[0]) if x[0] > 0 else 0 for x in dRes],alternative='two-sided'))\n",
    "    print('Medians:')\n",
    "    print(np.median([x[0] for x in sRes]),np.median([x[0] for x in dRes]))\n",
    "    print('Overall corr:')\n",
    "    print(spearmanr([x[4] for x in totRes],[x[0] for x in totRes]))\n",
    "    print()\n",
    "    print('GC content:')\n",
    "    print('Sing vs Dup comp:')\n",
    "    print(mannwhitneyu([x[1] for x in sRes],[x[1] for x in dRes],alternative='two-sided'))\n",
    "    print('Medians:')\n",
    "    print(np.median([x[1] for x in sRes]),np.median([x[1] for x in dRes]))\n",
    "    print('Overall corr:')\n",
    "    print(spearmanr([x[4] for x in totRes],[x[1] for x in totRes]))\n",
    "    print()\n",
    "    print('GC3 content:')\n",
    "    print('Sing vs Dup comp:')\n",
    "    print(mannwhitneyu([x[2] for x in sRes],[x[2] for x in dRes],alternative='two-sided'))\n",
    "    print('Medians:')\n",
    "    print(np.median([x[2] for x in sRes]),np.median([x[2] for x in dRes]))\n",
    "    print('Overall corr:')\n",
    "    print(spearmanr([x[4] for x in totRes],[x[2] for x in totRes]))\n",
    "    print()\n",
    "    print('Expression:')\n",
    "    sExpRes = [x for x in sRes if x[3]]\n",
    "    dExpRes = [x for x in dRes if x[3]]\n",
    "    totExpRes = [x for x in totRes if x[3]]\n",
    "    print('Sing vs Dup comp:')\n",
    "    print(mannwhitneyu([log10(x[3]) if x[3] > 0 else 0 for x in sExpRes],[log10(x[3]) if x[3] > 0 else 0 for x in dExpRes],alternative='two-sided'))\n",
    "    print('Medians:')\n",
    "    print(np.median([log10(x[3]) if x[3] > 0 else 0 for x in sExpRes]),np.median([log10(x[3]) if x[3] > 0 else 0 for x in dExpRes]))\n",
    "    print('Overall corr:')\n",
    "    print(spearmanr([x[4] for x in totExpRes],[x[3] for x in totExpRes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression on dN/dS\n",
    "#going to do OLS and LOWESS\n",
    "def doRegression(table):\n",
    "    cursor.execute('SELECT proxy_rate, cdsLen, gc, gc3, exp, dup_status FROM '+table+' WHERE excludedReason IS NULL AND proxy_dS < 4') \n",
    "    data = pd.DataFrame(cursor.fetchall())\n",
    "    data.columns = ['Rate','CDS','GC','GC3','Exp','DupStatus']\n",
    "    data = data[data['Exp']>0]\n",
    "    data['CDS'] = np.log10(data['CDS'])\n",
    "    data['Exp'] = np.log10(data['Exp'])\n",
    "    data['GC'] = data['GC']*100\n",
    "    data['GC3'] = data['GC3']*100\n",
    "    data['Rate'] = np.log10(data['Rate'])\n",
    "\n",
    "    dataDup = data[data['DupStatus']=='D']\n",
    "    dataSing = data[data['DupStatus']=='S']\n",
    "\n",
    "    for feature in ['CDS','GC','GC3','Exp']:\n",
    "        model = smf.ols('Rate ~ '+ feature, data=data)\n",
    "        res = model.fit()\n",
    "    #     print(res.summary())\n",
    "        residuals = res.resid\n",
    "        fittedVals = res.fittedvalues\n",
    "        data['linear_fit'+feature] = fittedVals\n",
    "        data['linear_resid'+feature] = residuals\n",
    "\n",
    "        print(feature)\n",
    "        print('Original comp:')\n",
    "        dOrig = data[data['DupStatus']=='D']['Rate']\n",
    "        sOrig = data[data['DupStatus']=='S']['Rate']\n",
    "        print(mannwhitneyu(sOrig,dOrig,alternative='two-sided'))\n",
    "        print('OLS residuals:')\n",
    "        dResid = data[data['DupStatus']=='D']['linear_resid'+feature]\n",
    "        sResid = data[data['DupStatus']=='S']['linear_resid'+feature]\n",
    "        print(mannwhitneyu(sResid,dResid,alternative='two-sided'))\n",
    "        print('LOWESS residuals:')\n",
    "        smoothModel = sm.nonparametric.lowess(data['Rate'],data[feature],frac=1/3,return_sorted=False)\n",
    "        data['lowess_fit'+feature] = smoothModel\n",
    "        data['lowess_resid'+feature] = data['Rate']-data['lowess_fit'+feature]\n",
    "\n",
    "\n",
    "        dResid = data[data['DupStatus']=='D']['lowess_resid'+feature]\n",
    "        sResid = data[data['DupStatus']=='S']['lowess_resid'+feature]\n",
    "        print(mannwhitneyu(sResid,dResid,alternative='two-sided'))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb742cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResidPlots(data):\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import probplot\n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(3,4,figsize=(30,20))\n",
    "    \n",
    "    scatterRow, histRow, normRow = axes\n",
    "    nameDict = {'CDS':'CDS length','Exp':'Expression level','GC':'GC content','GC3':'GC3 content'}\n",
    "    hPos = 0\n",
    "    for feat in ['CDS', 'Exp','GC','GC3']:\n",
    "        resids = data['linear_resid'+feat]\n",
    "        fitted = data['linear_fit'+feat]\n",
    "        #plot of resids vs rate\n",
    "        scatterRow[hPos].plot(fitted,resids,'bo')\n",
    "        scatterRow[hPos].set_xlabel('Fitted values',fontsize=16)\n",
    "        scatterRow[hPos].set_ylabel('Residuals',fontsize=16)\n",
    "        scatterRow[hPos].xaxis.set_tick_params(labelsize=16)\n",
    "        scatterRow[hPos].yaxis.set_tick_params(labelsize=16)\n",
    "        scatterRow[hPos].set_title(nameDict[feat],fontsize=18)\n",
    "        \n",
    "        #distribution of resids - possibly test of normality\n",
    "        histRow[hPos].hist(resids)\n",
    "        histRow[hPos].set_xlabel('Residuals',fontsize=16)\n",
    "        histRow[hPos].set_ylabel('Frequency',fontsize=16)\n",
    "        histRow[hPos].xaxis.set_tick_params(labelsize=16)\n",
    "        histRow[hPos].yaxis.set_tick_params(labelsize=16)\n",
    "        \n",
    "        \n",
    "        #QQ plot of resids\n",
    "        probplot(resids,dist='norm',plot=normRow[hPos])\n",
    "        normRow[hPos].xaxis.set_tick_params(labelsize=16)\n",
    "        normRow[hPos].yaxis.set_tick_params(labelsize=16)\n",
    "        normRow[hPos].set_ylabel('Ordered values',fontsize=16)\n",
    "        normRow[hPos].set_xlabel('Theoretical quantiles',fontsize=16)\n",
    "        normRow[hPos].set_title('')\n",
    "\n",
    "        hPos += 1\n",
    "    plt.savefig('residualDiagnostics.svg',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confounderBoxPlot(data,resids):\n",
    "    dataDup = data[data['DupStatus']=='D']\n",
    "    dataSing = data[data['DupStatus']=='S']\n",
    "    \n",
    "    fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "    sns.set()\n",
    "    for feat, ax in zip(['CDS','Exp','GC','GC3'],axes):\n",
    "        if resids == 'orig':\n",
    "            boxes1 = ax.boxplot([dataSing[feat],dataDup[feat]],patch_artist=True,labels=['Singleton','Duplicable'],flierprops={'ms':1})\n",
    "        elif resids == 'OLS':\n",
    "            boxes1 = ax.boxplot([dataSing['linear_resid'+feat],dataDup['linear_resid'+feat]],patch_artist=True,labels=['Singleton','Duplicable'],flierprops={'ms':1})\n",
    "        elif resids == 'lowess':\n",
    "            boxes1 = ax.boxplot([dataSing['lowess_resid'+feat],dataDup['lowess_resid'+feat]],patch_artist=True,labels=['Singleton','Duplicable'],flierprops={'ms':1})\n",
    "        boxes1['boxes'][0].set_fc('#DC3220')\n",
    "        boxes1['boxes'][1].set_fc('#005AB5')\n",
    "        for m in boxes1['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "\n",
    "        if feat == 'CDS':\n",
    "            ax.set_title('CDS length')\n",
    "            ax.set_ylabel('log(bp)')\n",
    "        elif feat == 'Exp':\n",
    "            ax.set_title('Expression')\n",
    "            ax.set_ylabel('log(TPM)')\n",
    "        elif feat == 'GC':\n",
    "            ax.set_title('% GC')\n",
    "            ax.set_ylabel('%')\n",
    "        elif feat == 'GC3':\n",
    "            ax.set_title('% GC3')\n",
    "            ax.set_ylabel('%')\n",
    "            \n",
    "        if resids != 'orig':\n",
    "            ax.set_ylabel('Residuals')\n",
    "    plt.savefig('confounderComp_'+resids+'.eps',bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ac74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionPlot(table,rType):\n",
    "# Plotting regression models\n",
    "    cursor.execute('SELECT proxy_rate, cdsLen, gc, gc3, exp, dup_status FROM '+table+' WHERE excludedReason IS NULL AND proxy_dS < 4') \n",
    "    data = pd.DataFrame(cursor.fetchall())\n",
    "    data.columns = ['Rate','CDS','GC','GC3','Exp','DupStatus']\n",
    "    data = data[data['Exp']>0]\n",
    "    data['CDS'] = np.log10(data['CDS'])\n",
    "    data['Exp'] = np.log10(data['Exp'])\n",
    "    data['GC'] = data['GC']*100\n",
    "    data['GC3'] = data['GC3']*100\n",
    "    data['Rate'] = np.log10(data['Rate'])\n",
    "\n",
    "    dataDup = data[data['DupStatus']=='D']\n",
    "    dataSing = data[data['DupStatus']=='S']\n",
    "\n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(2,2,figsize=(10,10),gridspec_kw={'hspace':0.35})\n",
    "    \n",
    "    \n",
    "    if rType == 'lowess':\n",
    "        # sns.regplot(x='Exp', y='Rate', data=data, scatter_kws={'s':0.5},ax=axes[0][0])\n",
    "        axes[0][0].plot(dataSing['CDS'],dataSing['Rate'],'o',color='#DC3220',ms=2)\n",
    "        axes[0][0].plot(dataDup['CDS'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='CDS',y='Rate',data=data,ax=axes[0][0],scatter=False,color='black',lowess=True)\n",
    "        sns.regplot(x='CDS',y='Rate',data=dataDup,ax=axes[0][0],scatter=False,color='#005AB5',truncate=False,lowess=True)\n",
    "        sns.regplot(x='CDS',y='Rate',data=dataSing,ax=axes[0][0],scatter=False,color='#DC3220',lowess=True)\n",
    "        axes[0][0].set_title('CDS length')\n",
    "        axes[0][0].set_xlabel('log(bp)')\n",
    "        \n",
    "        axes[0][1].plot(dataSing['Exp'],dataSing['Rate'],'o',color='#DC3220',ms=2)\n",
    "        axes[0][1].plot(dataDup['Exp'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='Exp',y='Rate',data=data,ax=axes[0][1],scatter=False,color='black',lowess=True)\n",
    "        sns.regplot(x='Exp',y='Rate',data=dataDup,ax=axes[0][1],scatter=False,color='#005AB5',truncate=False,lowess=True)\n",
    "        sns.regplot(x='Exp',y='Rate',data=dataSing,ax=axes[0][1],scatter=False,color='#DC3220',lowess=True)\n",
    "        axes[0][1].set_title('Expression')\n",
    "        axes[0][1].set_xlabel('log(TPM)')\n",
    "        \n",
    "        axes[1][0].plot(dataSing['GC'],dataSing['Rate'],'o',color='#DC3220',ms=2)\n",
    "        axes[1][0].plot(dataDup['GC'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='GC',y='Rate',data=data,ax=axes[1][0],scatter=False,color='black',lowess=True,label='All')\n",
    "        sns.regplot(x='GC',y='Rate',data=dataDup,ax=axes[1][0],scatter=False,color='#005AB5',truncate=False,lowess=True,label='Duplicable')\n",
    "        sns.regplot(x='GC',y='Rate',data=dataSing,ax=axes[1][0],scatter=False,color='#DC3220',lowess=True,label='Singletons')\n",
    "        axes[1][0].set_title('% GC')\n",
    "        axes[1][0].set_xlabel('%')\n",
    "        \n",
    "        axes[1][1].plot(dataSing['GC3'],dataSing['Rate'],'o',color='#DC3220',ms=2,fillstyle='none')\n",
    "        axes[1][1].plot(dataDup['GC3'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='GC3',y='Rate',data=data,ax=axes[1][1],scatter=False,color='black',lowess=True)\n",
    "        sns.regplot(x='GC3',y='Rate',data=dataDup,ax=axes[1][1],scatter=False,color='#005AB5',truncate=False,lowess=True)\n",
    "        sns.regplot(x='GC3',y='Rate',data=dataSing,ax=axes[1][1],scatter=False,color='#DC3220',lowess=True)\n",
    "        axes[1][1].set_title('% GC3')\n",
    "        axes[1][1].set_xlabel('%')\n",
    "    \n",
    "    elif rType == 'OLS':\n",
    "        axes[0][0].plot(dataSing['CDS'],dataSing['Rate'],'o',color='#DC3220',ms=2)\n",
    "        axes[0][0].plot(dataDup['CDS'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='CDS',y='Rate',data=data,ax=axes[0][0],scatter=False,color='black')\n",
    "        sns.regplot(x='CDS',y='Rate',data=dataDup,ax=axes[0][0],scatter=False,color='#005AB5',truncate=False)\n",
    "        sns.regplot(x='CDS',y='Rate',data=dataSing,ax=axes[0][0],scatter=False,color='#DC3220')\n",
    "        axes[0][0].set_title('CDS length')\n",
    "        axes[0][0].set_xlabel('log(bp)')\n",
    "        \n",
    "        axes[0][1].plot(dataSing['Exp'],dataSing['Rate'],'o',color='#DC3220',ms=2)\n",
    "        axes[0][1].plot(dataDup['Exp'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='Exp',y='Rate',data=data,ax=axes[0][1],scatter=False,color='black')\n",
    "        sns.regplot(x='Exp',y='Rate',data=dataDup,ax=axes[0][1],scatter=False,color='#005AB5',truncate=False)\n",
    "        sns.regplot(x='Exp',y='Rate',data=dataSing,ax=axes[0][1],scatter=False,color='#DC3220')\n",
    "        axes[0][1].set_title('Expression')\n",
    "        axes[0][1].set_xlabel('log(TPM)')\n",
    "\n",
    "        axes[1][0].plot(dataSing['GC'],dataSing['Rate'],'o',color='#DC3220',ms=2)\n",
    "        axes[1][0].plot(dataDup['GC'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='GC',y='Rate',data=data,ax=axes[1][0],scatter=False,color='black',label='All')\n",
    "        sns.regplot(x='GC',y='Rate',data=dataDup,ax=axes[1][0],scatter=False,color='#005AB5',truncate=False,label='Duplicable')\n",
    "        sns.regplot(x='GC',y='Rate',data=dataSing,ax=axes[1][0],scatter=False,color='#DC3220',label='Singleton')\n",
    "        axes[1][0].set_title('% GC')\n",
    "        axes[1][0].set_xlabel('%')\n",
    "\n",
    "        axes[1][1].plot(dataSing['GC3'],dataSing['Rate'],'o',color='#DC3220',ms=1.8)\n",
    "        axes[1][1].plot(dataDup['GC3'],dataDup['Rate'],'o',color='#005AB5',ms=2)\n",
    "        sns.regplot(x='GC3',y='Rate',data=data,ax=axes[1][1],scatter=False,color='black')\n",
    "        sns.regplot(x='GC3',y='Rate',data=dataDup,ax=axes[1][1],scatter=False,color='#005AB5',truncate=False)\n",
    "        sns.regplot(x='GC3',y='Rate',data=dataSing,ax=axes[1][1],scatter=False,color='#DC3220')\n",
    "        axes[1][1].set_title('% GC3')\n",
    "        axes[1][1].set_xlabel('%')\n",
    "        \n",
    "    axes[0][0].set_ylabel('log(dN/dS)')\n",
    "    axes[0][1].set_ylabel('log(dN/dS)')\n",
    "    axes[1][0].set_ylabel('log(dN/dS)')\n",
    "    axes[1][1].set_ylabel('log(dN/dS)')  \n",
    "    \n",
    "    axes[1][0].legend()\n",
    "    \n",
    "    plt.savefig(table+'_'+rType+'_final.eps',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monte carlo simulations to test sig of change in p value in rate comp when comparing residuals\n",
    "def residComp(feature, method, df, actualPVal, lower=True, it=1000):\n",
    "    import random,time\n",
    "    t = time.time()\n",
    "    count = 0\n",
    "    df2 = df.copy(deep=True)\n",
    "    if method == 'ols':\n",
    "        for i in range(it):\n",
    "            df2[feature] = np.random.permutation(df[feature].values)\n",
    "            model = smf.ols('Rate ~ '+ feature, data=df2)\n",
    "            res = model.fit()\n",
    "        #     print(res.summary())\n",
    "            residuals = res.resid\n",
    "            fittedVals = res.fittedvalues\n",
    "            df2['linear_fit'+feature] = fittedVals\n",
    "            df2['linear_resid'+feature] = residuals\n",
    "\n",
    "            dResid = df2[df2['Duplication_Status']=='D']['linear_resid'+feature]\n",
    "            sResid = df2[df2['Duplication_Status']=='S']['linear_resid'+feature]\n",
    "            p = mannwhitneyu(sResid,dResid,alternative='two-sided').pvalue\n",
    "            if lower:\n",
    "                if p <= actualPVal:\n",
    "                    count += 1\n",
    "            elif not lower:\n",
    "                if p >= actualPVal:\n",
    "                    count += 1\n",
    "            if i%10 == 0:\n",
    "                print(time.time()-t)\n",
    "    \n",
    "    \n",
    "    elif method == 'LOWESS':\n",
    "        for i in range(it):\n",
    "            df2[feature] = np.random.permutation(df[feature].values)\n",
    "#             display(df2[feature])\n",
    "            smoothModel = sm.nonparametric.lowess(df2['Rate'],df2[feature],frac=1/3,return_sorted=False)\n",
    "            df2['lowess_fit'+feature] = smoothModel\n",
    "            df2['lowess_resid'+feature] = df2['Rate']-df2['lowess_fit'+feature]\n",
    "            dResid = df2[df2['Duplication_Status']=='D']['lowess_resid'+feature]\n",
    "            sResid = df2[df2['Duplication_Status']=='S']['lowess_resid'+feature]\n",
    "            p = mannwhitneyu(sResid,dResid,alternative='two-sided').pvalue\n",
    "            if lower:\n",
    "                if p <= actualPVal:\n",
    "                    count += 1\n",
    "            elif not lower:\n",
    "                if p >= actualPVal:\n",
    "                    count += 1\n",
    "#             if i%10 == 0:\n",
    "#                 print(time.time()-t)\n",
    "    return count/it\n",
    "\n",
    "print(residComp('CDS_Length','LOWESS',df,0.05287523357330763,lower=False))\n",
    "print(residComp('Expression','LOWESS',df,0.010352820114804085))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bfd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRT(lnLAlt, lnLNull, numParamAlt, numParamNull):\n",
    "#     lnLNull =  -1462.62\n",
    "#     lnLAlt =  -1460.62\n",
    "    chi_crit = 2*(max([lnLAlt,lnLNull])-min([lnLAlt,lnLNull]))\n",
    "    dof = numParamAlt - numParamNull\n",
    "    p = chi2.sf(chi_crit,dof)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d624b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_species(node):\n",
    "        return node.name[-4:]\n",
    "def rateHypTest(table,alignmentDir,groupTable):\n",
    "    cursor.execute('SELECT groupID, groupMembers FROM '+groupTable)\n",
    "    groupDict = dict(cursor.fetchall())\n",
    "    cursor.execute('SELECT id, tree FROM '+table+' WHERE excludedReason IS NULL AND dup_status == \"D\"')\n",
    "    r = cursor.fetchall()\n",
    "    accPvals, asymmPvals = [],[]\n",
    "    accLs, asymmLs = [],[]\n",
    "    accRates = []\n",
    "    idList = []\n",
    "    for gene, origTree in tqdm(r):\n",
    "        tree = PhyloTree(origTree,sp_naming_function=get_species)\n",
    "        #make condon alignment,as for rateCalc\n",
    "        #for each sequence\n",
    "        cdsDict = {}\n",
    "        for node in tree.traverse():\n",
    "                if node.name == '' or node.name.isnumeric():\n",
    "                    continue\n",
    "                ID = retrieve_prot_id(node)\n",
    "                sp = retrieve_species(node)\n",
    "                #get CDS sequence\n",
    "                cmdCDS = ['sed', '-n', '-e', '/'+ ID +'/,/>/ p', 'dipteraCDS_raw/'+sp+'_raw_cds.fna']\n",
    "                pCDS = Popen(cmdCDS, stdout=PIPE,stderr=PIPE)\n",
    "                out,err = pCDS.communicate()\n",
    "\n",
    "                CDSseq = out.split(bytes('\\n','utf-8'))[1:-2]\n",
    "                CDSseq = ''.join([x.decode('utf-8') for x in CDSseq])\n",
    "                #store CDS seqs and headers in dict\n",
    "                cdsDict['>'+ID + ':' + sp] = CDSseq\n",
    "        alignFile = alignmentDir + '/group' + [str(x) for x in groupDict if ID in groupDict[x]][0] + '_alignment.fa'\n",
    "        #write only relevant sequences to prot/CDS files, in same order with same headers\n",
    "        with open(alignFile,'r') as prot_file, open('interCDS.fa','w') as cds_file, open('interProt.fa','w') as prot_align_file:\n",
    "        #  muscle outputs alignments in a different order to input sequences\n",
    "        #  cds sequences have to be written in same order to work with pal2nal\n",
    "            alignSeq = ''\n",
    "            alignDict = {}\n",
    "            #put entire protein alignment in a string\n",
    "            for line in prot_file:\n",
    "                alignSeq = alignSeq + line\n",
    "            \n",
    "            #get the order the headers occur in\n",
    "            order= [line for line in alignSeq.split('\\n') if line.startswith('>')]\n",
    "            #make new headers based on this\n",
    "            #order2 = ['>'+retrieve_prot_id_align(x)+':'+retrieve_species(x) for x in order]\n",
    "            order2=order\n",
    "    #       create new protein alignment file with correct headers and in correct order\n",
    "    #       store in dict\n",
    "            incl = False \n",
    "            for line in alignSeq.split('\\n'):\n",
    "                if line.startswith('>'):\n",
    "#                     newHead = '>'+retrieve_prot_id_align(line)+':'+retrieve_species(line)\n",
    "                    newHead = line\n",
    "                    if newHead in order2:\n",
    "                        alignDict[newHead] = ''\n",
    "                        current = newHead\n",
    "                        incl = True\n",
    "                    else:\n",
    "                        incl = False\n",
    "                elif incl == True:\n",
    "                    alignDict[current] = alignDict[current] + line\n",
    "            # get CDS in right order as well, write each header and sequence to CDS/prot file\n",
    "            for x in order2:\n",
    "                try:\n",
    "                    i = [y for y in cdsDict if retrieve_prot_id_align(x) in y][0]\n",
    "                    seq = cdsDict[i]\n",
    "                except IndexError:\n",
    "                    continue #species not included in pruned trees ...I guess this is where I'm filtering??? -Looks that way\n",
    "                \n",
    "                cds_file.write(x.replace('|','_') + '\\n')\n",
    "                cds_file.write(seq + '\\n')\n",
    "                prot_align_file.write(x.replace('|','_') + '\\n')\n",
    "                prot_align_file.write(alignDict[x] + '\\n')     \n",
    "        # somehow convert to codon alignments - pal2nal\n",
    "        #     usage: pal2nal prot_alignment dna_seq -output paml\n",
    "    #     print('at pal2nal')\n",
    "        cm = '../SOFTWARE/pal2nal.pl interProt.fa interCDS.fa -output paml'\n",
    "        p2 = Popen(cm.split(' '), stdout=PIPE, stderr=PIPE)\n",
    "        out2, err2 = p2.communicate()\n",
    "\n",
    "        with open('interCodonAlign.paml','w') as cod_file:\n",
    "            cod_file.write(out2.decode())\n",
    "     \n",
    "        alignmentFile = '~/drosphilaFinal/interCodonAlign.paml'   \n",
    "        # Getting to the actual meat of the thing, get dup events from the tree\n",
    "        events = tree.get_descendant_evol_events()\n",
    "        dupEvents = [e for e in events if e.etype == 'D']\n",
    "        \n",
    "        numEvents = 0\n",
    "        \n",
    "        for e in dupEvents:\n",
    "            #I think I have to do this every round to get the unedited tree to start with again\n",
    "            tree = PhyloTree(origTree,sp_naming_function=get_species)\n",
    "            # each of these two will have different rates in the asymmetry alt model\n",
    "            dupGroup1 = e.in_seqs \n",
    "            dupGroup2 = e.out_seqs\n",
    "            #all of these will have the same rate in the null for asymmetry and the alt for acceleration\n",
    "            totalPostDup = dupGroup1.union(dupGroup2) \n",
    "            #write tree to temp treefile for null acceleration model, run PAML, extract lnL and params\n",
    "            tree.write(outfile='temp.treefile')\n",
    "\n",
    "            cmd = codeml.Codeml(alignment='interCodonAlign.paml', tree='temp.treefile', out_file='results.out',working_dir='.')\n",
    "            cmd.read_ctl_file('model_zero.ctl')\n",
    "            output = cmd.run(command='../SOFTWARE/paml4.9j/bin/codeml')\n",
    "            allOneRate_lnL = output['NSsites'][0]['lnL']\n",
    "            try:\n",
    "                allOneRate_numParams = len(output['NSsites'][0]['parameters']['omega'])\n",
    "            except TypeError:\n",
    "                allOneRate_numParams = 1\n",
    "\n",
    "    #   edit tree to label all post dup branches with one rate, write to file, run PAML, extract lnL and params\n",
    "            for node in tree.traverse():\n",
    "                if not node.is_leaf():\n",
    "                    node.name = ' ' #I may or may not need this, ete is labelling these all with the name NoName if name = ''\n",
    "                    #I don't know if that poses an issue to PAML\n",
    "            nodeToLabel = tree.get_common_ancestor(totalPostDup)\n",
    "            nodeToLabel.name = ' $1'\n",
    "            tree.write(format=8,outfile='temp.treefile')\n",
    "\n",
    "            cmd = codeml.Codeml(alignment='interCodonAlign.paml', tree='temp.treefile', out_file='results.out',working_dir='.')\n",
    "            cmd.read_ctl_file('model_acc.ctl')\n",
    "            output = cmd.run(command='../SOFTWARE/paml4.9j/bin/codeml')\n",
    "            postDupRate_lnL = output['NSsites'][0]['lnL']\n",
    "            postDupRate_numParams = len(output['NSsites'][0]['parameters']['omega'])\n",
    "            postDupRate_rates = output['NSsites'][0]['parameters']['omega']\n",
    "            \n",
    "            #reset tree\n",
    "            tree = PhyloTree(origTree,sp_naming_function=get_species)\n",
    "        #   edit tree to label the post dup branches with 2 different rates, write to file, run PAML, extract lnL and params\n",
    "            for node in tree.traverse():\n",
    "                if not node.is_leaf():\n",
    "                    node.name = ' ' #I may or may not need this, ete is labelling these all with the name NoName if name = ''\n",
    "                    #I don't know if that poses an issue to PAML\n",
    "            if len(dupGroup1) >1:\n",
    "                nodeToLabel = tree.get_common_ancestor(dupGroup1)\n",
    "                nodeToLabel.name = ' $1'\n",
    "            else:\n",
    "                name = dupGroup1.pop()\n",
    "                node = tree&name\n",
    "                node.name = node.name + ' #1'\n",
    "                \n",
    "            if len(dupGroup2) >1:\n",
    "                nodeToLabel = tree.get_common_ancestor(dupGroup2)\n",
    "                nodeToLabel.name = ' $2'\n",
    "            else:\n",
    "                name = dupGroup2.pop()\n",
    "                node = tree&name\n",
    "                node.name = node.name + ' #2'\n",
    "                \n",
    "            tree.write(format=8,outfile='temp.treefile')\n",
    "\n",
    "            cmd = codeml.Codeml(alignment='interCodonAlign.paml', tree='temp.treefile', out_file='results.out',working_dir='.')\n",
    "            cmd.read_ctl_file('model_asym.ctl')\n",
    "            output = cmd.run(command='../SOFTWARE/paml4.9j/bin/codeml')\n",
    "            asymmRate_lnL = output['NSsites'][0]['lnL']\n",
    "            asymmRate_numParams = len(output['NSsites'][0]['parameters']['omega'])\n",
    "        \n",
    "    #         #do LRT for the various combinations\n",
    "            try:\n",
    "                accLRT = LRT(postDupRate_lnL,allOneRate_lnL,postDupRate_numParams,allOneRate_numParams)\n",
    "                asymLRT = LRT(asymmRate_lnL,postDupRate_lnL,asymmRate_numParams,postDupRate_numParams)\n",
    "            except ZeroDivisionError:\n",
    "                print('Zero Divide?')\n",
    "                continue\n",
    "\n",
    "            #append pvals for final FDR correction\n",
    "            accPvals.append(accLRT)\n",
    "            asymmPvals.append(asymLRT)\n",
    "            accLs.append((allOneRate_lnL,postDupRate_lnL))\n",
    "            asymmLs.append((postDupRate_lnL,asymmRate_lnL))\n",
    "            accRates.append(postDupRate_rates)\n",
    "            idList.append(gene)\n",
    "           \n",
    "    #FDR pval adjustment\n",
    "    accFDR = fdr(accPvals) #default alpha is 0.05\n",
    "    asymFDR = fdr(asymmPvals)\n",
    "    \n",
    "    accPvalsAdjusted = [x for x in accFDR[1]] \n",
    "    asymPvalsAdjusted = [x for x in asymFDR[1]]\n",
    "    \n",
    "    \n",
    "    return {'ID list':idList,\n",
    "            'Unadjusted P vals, acceleration':accPvals,\n",
    "            'Adjusted P vals, acceleration':accPvalsAdjusted,\n",
    "           'Unadjusted P vals, asymmetry':asymmPvals,\n",
    "            'Adjusted P vals, asymmetry':asymPvalsAdjusted,\n",
    "           'Null v Alt lnL, acceleration':accLs,\n",
    "           'Null v alt lnL, asymmetry':asymmLs,\n",
    "           'Accelerate Model rates':accRates}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defFES(table,filename=None):\n",
    "    # fast evolving singletons\n",
    "    # define the group\n",
    "    try:\n",
    "        cursor.execute('ALTER TABLE '+table+' ADD COLUMN FES TEXT')\n",
    "    except:\n",
    "        pass\n",
    "    cursor.execute('SELECT id, dup_status, proxy_rate FROM '+ table +' WHERE excludedReason IS NULL AND proxy_dS < 4')\n",
    "    res = cursor.fetchall()\n",
    "    rates = [x[2] for x in res]\n",
    "    cutoff = np.percentile(rates,95)\n",
    "    print('95th %ile:',cutoff)\n",
    "    fes = [x[0] for x in res if x[2] >= cutoff and x[1]=='S']\n",
    "    dups = [x[0] for x in res if x[2] >= cutoff and x[1]=='D']\n",
    "    for s in fes:\n",
    "        cursor.execute('UPDATE '+table+' SET FES = \"T\" WHERE id == ?',(s,))\n",
    "    db.commit()\n",
    "    if filename:\n",
    "        with open(filename,'w') as out:\n",
    "            for s in fes:\n",
    "                out.write(s+'\\n')\n",
    "    print('Dups in top 5%',len(dups))\n",
    "    return fes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522903d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxRestrictionLevels(fes,table):\n",
    "    restrictDict = {1:['BCOP','AALB','ASTE','AAEG','AEAL','CPIP','CQUI'],2:['HILL'],3:['CCAP','BTRY'],\n",
    "               4:['SLEB'],5:['DVIR','DNOV','DHYD','DALB','DBUS'],6:['DSUB','DGUA','DPER','DPSE','DMIR'],\n",
    "               7:['DANA'],8:['DSER','DKIK'],9:['DFIC','DELE'],10:['DBIA','DSUP','DSUZ']}\n",
    "    levelList = []\n",
    "    with open('restrictionLevels_'+table+'.txt','w') as out:\n",
    "        for s in fes:\n",
    "            cursor.execute('SELECT baseTree FROM '+table+' WHERE id == ?',(s,))\n",
    "            tree = cursor.fetchall()[0][0]\n",
    "            for level in restrictDict:\n",
    "                for sp in restrictDict[level]:\n",
    "                    if sp in tree:\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                r = level\n",
    "                levelList.append(r)\n",
    "                tree = PhyloTree(tree)\n",
    "                for node in tree.traverse():\n",
    "                    #I need DMEL ids for abSENSE comp\n",
    "                    if 'DMEL' in node.name:\n",
    "                        g = node.name\n",
    "                out.write(g+'\\t'+str(level)+'\\n')\n",
    "                break\n",
    "    return levelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27172f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMissedDups(blastDir,table,fesList):\n",
    "    #checking for blast hits within the sp of interest\n",
    "    #convert to Orthofinder IDs\n",
    "    #for each one, check the right file, see if has a non-self hit\n",
    "        #record all the species that have a dup for this group\n",
    "    orthoIDDict = {}\n",
    "    with open(blastDir+'/SequenceIDs.txt','r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            line[0] = line[0].strip(':')\n",
    "            line[1] = line[1].replace('|','_')\n",
    "            orthoIDDict[line[1]] = line[0]\n",
    "#     blastDir = './dipteraTranslations_processed/OrthoFinder/Results_Mar30/WorkingDirectory/'\n",
    "    for s in fesList:\n",
    "        dupInSp = []\n",
    "        possDup = False\n",
    "        possParas = []\n",
    "        cursor.execute('SELECT tree FROM '+table+' WHERE id == ?',(s,))\n",
    "        #make list of the genes involved\n",
    "        tree = cursor.fetchall()[0][0]\n",
    "        tree = PhyloTree(tree)\n",
    "        genes = []\n",
    "        for node in tree.traverse():\n",
    "            if node.name == '' or node.name.isnumeric():\n",
    "                continue\n",
    "            else:\n",
    "                genes.append(node.name)\n",
    "        for gene in genes:\n",
    "            geneO = orthoIDDict[gene]\n",
    "            spO = geneO.split('_')[0]\n",
    "            blastFile = blastDir+'Blast'+spO+'_'+spO+'.txt.gz'\n",
    "            p1 = Popen(['gunzip','-c',blastFile],stdout=PIPE,stderr=PIPE)\n",
    "    #         p2 = Popen(['awk','{if($11<=0.0001) print $0}'],stdin=p1.stdout,stdout=PIPE,stderr=PIPE)\n",
    "            p3 = Popen(['grep',geneO+'\\s'],stdin=p1.stdout,stdout=PIPE,stderr=PIPE)\n",
    "            out, err = p3.communicate()\n",
    "            bRes = [x for x in out.decode().split('\\n') if x != '']\n",
    "            if len(bRes) > 1:\n",
    "                possDup = True\n",
    "                for x in bRes:\n",
    "                    x = x.split('\\t')\n",
    "                    if x[0] == x[1]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if x[0] == geneO:\n",
    "                            possP = x[1]\n",
    "                        elif x[1] == geneO:\n",
    "                            possP = x[0]\n",
    "                        possP = [y for y in orthoIDDict if orthoIDDict[y]==possP][0]\n",
    "                        possParas.append(possP)\n",
    "        if possDup:\n",
    "            possParas=','.join(possParas)\n",
    "            cursor.execute('UPDATE TABLE '+table+' SET possMissedDup=\"T\" WHERE id == ?',(s,))\n",
    "            cursor.execute('UPDATE TABLE '+table+' SET possMissedParas = ? WHERE id == ?',(possParas,s))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fesConfoundComp():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2420f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDupSpDist(table)\n",
    "    cursor.execute('SELECT dupInSp FROM '+table+' WHERE excludedReason IS NULL AND dup_status == \"D\"')\n",
    "    r = [x[0] for x in cursor.fetchall()]\n",
    "    return Counter(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('CREATE INDEX seqID ON sequenceTab(id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fastas for the newly created Orthofinder groups and SonicParanoid, for the singleton groups\n",
    "# createGroupFastas(extractSingGroups('groups_Orthofinder',sing_list),'orthofinderGroupFastas','groups_Orthofinder')\n",
    "# createGroupFastas(extractSingGroups('groups_SonicParanoid',sing_list),'sonicParaGroupFastas','groups_SonicParanoid')\n",
    "# createGroupFastas(extractSingGroups('groups_Orthofinder_ultrasens',sing_list),'orthofinderUltraGroupFastas','groups_Orthofinder_ultrasens')\n",
    "#createGroupFastas(extractSingGroups('groups_Orthofinder',sing_list_relaxed),'orthofinderGroupFastasRel','groups_Orthofinder')\n",
    "createGroupFastas(extractSingGroups('groups_Orthofinder',sing_list_all),'orthofinderGroupFastas','groups_Orthofinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c869926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#muscle alignments for each set of groups\n",
    "# doGroupAlignment('orthofinderAlignments','orthofinderGroupFastas',singList=None)\n",
    "# doGroupAlignment('sonicParaAlignments','sonicParaGroupFastas',singList=None)\n",
    "# doGroupAlignment('omaAlignments','dipteraOMA/Output/OrthologousGroupsFasta',singList=sing_list,groupTable='groups_OMA')\n",
    "# doGroupAlignment('omaAlignments_lowerThresh','omaOutput2/OrthologousGroupsFasta',singList=sing_list,groupTable='groups_OMA_lowerThresh')\n",
    "# doGroupAlignment('omaAlignments_higherThresh','omaOutput3/OrthologousGroupsFasta',singList=sing_list,groupTable='groups_OMA_higherThresh')\n",
    "# doGroupAlignment('orthofinderUltraAlignments','orthofinderUltraGroupFastas',singList=None)\n",
    "doGroupAlignment('orthofinderAlignments','orthofinderGroupFastas',singList=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree building\n",
    "# buildTrees('orthofinderAlignments')\n",
    "# buildTrees('omaAlignments')\n",
    "# buildTrees('omaAlignments_lowerThresh')\n",
    "# buildTrees('omaAlignments_higherThresh')\n",
    "# buildTrees('sonicParaAlignments')\n",
    "# buildTrees('orthofinderUltraAlignments')\n",
    "buildTrees('orthofinderAlignments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aeaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial data entry, trees for singleton groups\n",
    "# processTrees('orthofinderAlignments',sing_list,'singTrees_Orthofinder','groups_Orthofinder')\n",
    "processTrees('orthofinderUltraAlignments',sing_list, 'singTrees_Orthofinder_ultrasens', 'groups_Orthofinder_ultrasens')\n",
    "# processTrees('sonicParaAlignments',sing_list,'singTrees_SonicParanoid','groups_SonicParanoid')\n",
    "# processTrees('omaAlignments',sing_list,'singTrees_OMA','groups_OMA')\n",
    "# processTrees('omaAlignments_lowerThresh',sing_list,'singTrees_OMA_lowerThresh','groups_OMA_lowerThresh')\n",
    "# processTrees('omaAlignments_higherThresh',sing_list,'singTrees_OMA_higherThresh','groups_OMA_higherThresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97caee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering and checks for Orthofinder\n",
    "# checkMissingSp('singTrees_Orthofinder')\n",
    "# checkOutgroupDup('singTrees_Orthofinder')\n",
    "\n",
    "#filtering and checks for Orthofinder\n",
    "checkMissingSp('singTrees_Orthofinder_ultrasens')\n",
    "checkOutgroupDup('singTrees_Orthofinder_ultrasens')\n",
    "\n",
    "# filtering and checks for SonicParanoid\n",
    "# checkMissingSp('singTrees_SonicParanoid')\n",
    "# checkOutgroupDup('singTrees_SonicParanoid')\n",
    "\n",
    "\n",
    "#filtering and checks for OMA (defaults)\n",
    "# checkMissingSp('singTrees_OMA')\n",
    "# checkOutgroupDup('singTrees_OMA')\n",
    "\n",
    "\n",
    "#filtering and checks for OMA (lower inParalogTol value)\n",
    "# checkMissingSp('singTrees_OMA_lowerThresh')\n",
    "# checkOutgroupDup('singTrees_OMA_lowerThresh')\n",
    "\n",
    "#filtering and checks for OMA (higher inParalogTol value)\n",
    "# checkMissingSp('singTrees_OMA_higherThresh')\n",
    "# checkOutgroupDup('singTrees_OMA_higherThresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6eda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkSplits('singTrees_Orthofinder')\n",
    "checkSplits('singTrees_Orthofinder_ultrasens')\n",
    "# checkSplits('singTrees_SonicParanoid')\n",
    "# checkSplits('singTrees_OMA')\n",
    "# checkSplits('singTrees_OMA_lowerThresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e855df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting trees, final table creation - in individual cells because I don't want to do all this manual checking at once\n",
    "doSplits('singTrees_Orthofinder','procTrees_Orthofinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd004dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doSplits('singTrees_SonicParanoid','procTrees_SonicParanoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doSplits('singTrees_OMA','procTrees_OMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c85d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "doSplits('singTrees_OMA_lowerThresh','procTrees_OMA_lowerThresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doSplits('singTrees_OMA_higherThresh','procTrees_OMA_higherThresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e992563",
   "metadata": {},
   "outputs": [],
   "source": [
    "doSplits('singTrees_Orthofinder_relaxed','procTrees_Orthofinder_relaxed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54feafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering final table, rate calculation\n",
    "#assignDupStatus('procTrees_Orthofinder')\n",
    "#checkDupTiming('procTrees_Orthofinder')\n",
    "#checkCorrectOutgroups('procTrees_Orthofinder')\n",
    "# rateCalc('procTrees_Orthofinder','orthofinderAlignments','singTrees_Orthofinder')\n",
    "\n",
    "# assignDupStatus('procTrees_Orthofinder_ultrasens')\n",
    "# checkDupTiming('procTrees_Orthofinder_ultrasens')\n",
    "# checkCorrectOutgroups('procTrees_Orthofinder_ultrasens')\n",
    "# rateCalc('procTrees_Orthofinder_ultrasens','orthofinderUltraAlignments','singTrees_Orthofinder_ultrasens')\n",
    "\n",
    "assignDupStatus('procTrees_Orthofinder_relaxed')\n",
    "checkDupTiming('procTrees_Orthofinder_relaxed')\n",
    "checkCorrectOutgroups('procTrees_Orthofinder_relaxed')\n",
    "rateCalc('procTrees_Orthofinder_relaxed','orthofinderAlignments','singTrees_Orthofinder_relaxed')\n",
    "\n",
    "# assignDupStatus('procTrees_SonicParanoid')\n",
    "# checkDupTiming('procTrees_SonicParanoid')\n",
    "# checkCorrectOutgroups('procTrees_SonicParanoid')\n",
    "# rateCalc('procTrees_SonicParanoid','sonicParaAlignments','singTrees_SonicParanoid')\n",
    "\n",
    "# assignDupStatus('procTrees_OMA')\n",
    "# checkDupTiming('procTrees_OMA')\n",
    "# checkCorrectOutgroups('procTrees_OMA')\n",
    "# rateCalc('procTrees_OMA','omaAlignments','singTrees_OMA')\n",
    "\n",
    "# assignDupStatus('procTrees_OMA_lowerThresh')\n",
    "# checkDupTiming('procTrees_OMA_lowerThresh')\n",
    "# checkCorrectOutgroups('procTrees_OMA_lowerThresh')\n",
    "# rateCalc('procTrees_OMA_lowerThresh','omaAlignments_lowerThresh','singTrees_OMA_lowerThresh')\n",
    "\n",
    "# assignDupStatus('procTrees_OMA_higherThresh')\n",
    "# checkDupTiming('procTrees_OMA_higherThresh')\n",
    "# checkCorrectOutgroups('procTrees_OMA_higherThresh')\n",
    "# rateCalc('procTrees_OMA_higherThresh','omaAlignments_higherThresh','singTrees_OMA_higherThresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rateComp('procTrees_Orthofinder','confirm',limitDS=False)\n",
    "rateComp('procTrees_Orthofinder','proxy',limitDS=False)\n",
    "\n",
    "rateComp('procTrees_Orthofinder','confirm',limitDS=True)\n",
    "rateComp('procTrees_Orthofinder','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ff41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rateComp('procTrees_Orthofinder_relaxed','confirm',limitDS=False)\n",
    "rateComp('procTrees_Orthofinder','proxy',limitDS=False)\n",
    "\n",
    "rateComp('procTrees_Orthofinder_relaxed','confirm',limitDS=True)\n",
    "rateComp('procTrees_Orthofinder_relaxed','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "rateComp('procTrees_Orthofinder_ultrasens','confirm',limitDS=False)\n",
    "rateComp('procTrees_Orthofinder_ultrasens','proxy',limitDS=False)\n",
    "\n",
    "rateComp('procTrees_Orthofinder_ultrasens','confirm',limitDS=True)\n",
    "rateComp('procTrees_Orthofinder_ultrasens','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rateComp('procTrees_SonicParanoid','confirm',limitDS=False)\n",
    "rateComp('procTrees_SonicParanoid','proxy',limitDS=False)\n",
    "\n",
    "rateComp('procTrees_SonicParanoid','confirm',limitDS=True)\n",
    "rateComp('procTrees_SonicParanoid','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "rateComp('procTrees_OMA','confirm',limitDS=False)\n",
    "rateComp('procTrees_OMA','proxy',limitDS=False)\n",
    "\n",
    "rateComp('procTrees_OMA','confirm',limitDS=True)\n",
    "rateComp('procTrees_OMA','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceea2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateRateCompFigure('procTrees_Orthofinder','confirm',limitDS=False)\n",
    "generateRateCompFigure('procTrees_Orthofinder','proxy',limitDS=False)\n",
    "\n",
    "generateRateCompFigure('procTrees_Orthofinder','confirm',limitDS=True)\n",
    "generateRateCompFigure('procTrees_Orthofinder','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateRateCompFigure('procTrees_Orthofinder_relaxed','confirm',limitDS=False)\n",
    "generateRateCompFigure('procTrees_Orthofinder_relaxed','proxy',limitDS=False)\n",
    "\n",
    "generateRateCompFigure('procTrees_Orthofinder_relaxed','confirm',limitDS=True)\n",
    "generateRateCompFigure('procTrees_Orthofinder_relaxed','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc45a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateRateCompFigure('procTrees_Orthofinder_ultrasens','confirm',limitDS=False)\n",
    "generateRateCompFigure('procTrees_Orthofinder_ultrasens','proxy',limitDS=False)\n",
    "\n",
    "generateRateCompFigure('procTrees_Orthofinder_ultrasens','confirm',limitDS=True)\n",
    "generateRateCompFigure('procTrees_Orthofinder_ultrasens','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateRateCompFigure('procTrees_SonicParanoid','confirm',limitDS=False)\n",
    "generateRateCompFigure('procTrees_SonicParanoid','proxy',limitDS=False)\n",
    "\n",
    "generateRateCompFigure('procTrees_SonicParanoid','confirm',limitDS=True)\n",
    "generateRateCompFigure('procTrees_SonicParanoid','proxy',limitDS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confounderInsert('procTrees_Orthofinder')\n",
    "confounderCompCorr('procTrees_Orthofinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressDF = doRegression('procTrees_Orthofinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17131575",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionPlot('procTrees_Orthofinder','OLS')\n",
    "regressionPlot('procTrees_Orthofinder','lowess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confounderBoxPlot(regressDF,'orig')\n",
    "confounderBoxPlot(regressDF,'OLS')\n",
    "confounderBoxPlot(regressDF,'lowess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73778ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monte carlo sim to see if change in p val is significant\n",
    "# print('CDS')\n",
    "print(residComp('CDS', 'OLS', regressDF, 0.00046949663863196666, lower=True, it=100000))\n",
    "# print('Expression')\n",
    "print(residComp('Exp', 'OLS', regressDF, 0.00013165187064657547, lower=True, it=100000))\n",
    "print('GC')\n",
    "print(residComp('GC', 'OLS', regressDF, 0.017101905456286735, lower=False, it=100000))\n",
    "print('GC3')\n",
    "print(residComp('Exp', 'OLS', regressDF, 0.04546184449416842, lower=False, it=100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "resDict = rateHypTest('procTrees_Orthofinder','orthofinderAlignments','groups_Orthofinder')\n",
    "accRates = [x for x in zip(resDict['Adjusted P vals, acceleration'],resDict['Accelerate Model rates'])]\n",
    "print('Higher post dup rate count:',len([x for x in accRates if x[1][1]>x[1][0]]))\n",
    "accRatesUnadj = [x for x in zip(resDict['Unadjusted P vals, acceleration'],resDict['Accelerate Model rates'])]\n",
    "print('Higher rate, sig before correction:',len([x for x in accRatesUnadj if x[0] < 0.05 and x[1][1]>x[1][0]]))\n",
    "print('Higher rate, sig after correction:',len([x for x in accRates if x[0] < 0.05 and x[1][1]>x[1][0]]))\n",
    "\n",
    "asymRates = [x for x in resDict['Adjusted P vals, asymmetry']]\n",
    "asymRatesUnadj = [x for x in resDict['Unadjusted P vals, asymmetry']]\n",
    "print('Asym rate, sig before correction:',len([x for x in asymRatesUnadj if x[0] < 0.05]))\n",
    "print('Asym rate, sig after correction:',len([x for x in asymRates if x[0] < 0.05]))\n",
    "print()\n",
    "print('Fisher\\'s method for combining p values')\n",
    "print('Acceleration:')\n",
    "print(combine_pvalues(accRatesUnadj).pval)\n",
    "print('Asymmetry:')\n",
    "print(combine_pvalues(asymRatesUnadj).pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f79418",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkMissedDups('dipteraTranslations_processed/OrthoFinder/Results_Mar30/WorkingDirectory','procTrees_Orthofinder',defFES('procTrees_Orthofinder'))\n",
    "checkMissedDups('dipteraTranslations_processed/OrthoFinder/Results_May28/WorkingDirectory','procTrees_Orthofinder_ultrasens',defFES('procTrees_Orthofinder_ultrasens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28838dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for blast hits within the sp of interest\n",
    "orthoIDDict = {}\n",
    "with open('dipteraTranslations_processed/OrthoFinder/Results_Mar30/WorkingDirectory/SequenceIDs.txt','r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip('\\n').split(' ')\n",
    "        line[0] = line[0].strip(':')\n",
    "        line[1] = line[1].replace('|','_')\n",
    "        orthoIDDict[line[1]] = line[0]\n",
    "blastDir = './dipteraTranslations_processed/OrthoFinder/Results_Mar30/WorkingDirectory/'\n",
    "for s in fes:\n",
    "    dupInSp = []\n",
    "    cursor.execute('SELECT tree FROM procTrees_Orthofinder WHERE id == ?',(s,))\n",
    "    #make list of the genes involved\n",
    "    tree = cursor.fetchall()[0][0]\n",
    "    tree = PhyloTree(tree)\n",
    "    genes = []\n",
    "    for node in tree.traverse():\n",
    "        if node.name == '' or node.name.isnumeric():\n",
    "            continue\n",
    "        else:\n",
    "            genes.append(node.name)\n",
    "    for gene in genes:\n",
    "        geneO = orthoIDDict[gene]\n",
    "        spO = geneO.split('_')[0]\n",
    "        blastFile = blastDir+'Blast'+spO+'_'+spO+'.txt.gz'\n",
    "        p1 = Popen(['gunzip','-c',blastFile],stdout=PIPE,stderr=PIPE)\n",
    "#         p2 = Popen(['awk','{if($11<=0.0001) print $0}'],stdin=p1.stdout,stdout=PIPE,stderr=PIPE)\n",
    "        p3 = Popen(['grep',geneO+'\\s'],stdin=p1.stdout,stdout=PIPE,stderr=PIPE)\n",
    "        out, err = p3.communicate()\n",
    "        \n",
    "        if len([x for x in out.decode().split('\\n') if x != '']) > 1:\n",
    "            print('dup')\n",
    "            print([x for x in out.decode().split('\\n') if x != ''])\n",
    "        \n",
    "    #convert to Orthofinder IDs\n",
    "    #for each one, check the right file, see if has a non-self hit\n",
    "        #record all the species that have a dup for this group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots for comparing possible confounders within fast-evolving singletons\n",
    "fes = defFES('procTrees_Orthofinder')\n",
    "cursor.execute('SELECT id, cdsLen, gc3, exp FROM procTrees_Orthofinder WHERE excludedReason IS NULL AND dup_status == \"S\" AND proxy_dS < 4')\n",
    "res = cursor.fetchall()\n",
    "fesFeats = [x for x in res if x[0] in fes]\n",
    "otherFeats = [x for x in res if x[0] not in fes]\n",
    "\n",
    "cursor.execute('SELECT id, cdsLen, gc3, exp FROM procTrees_Orthofinder WHERE excludedReason IS NULL AND dup_status == \"D\" AND proxy_dS < 4')\n",
    "dupFeats = cursor.fetchall()\n",
    "\n",
    "\n",
    "fig,axes= plt.subplots(1,3,figsize=(15,5))\n",
    "ax1,ax2,ax3 = axes\n",
    "\n",
    "ax1.set_title('CDS Length')\n",
    "ax1.set_ylabel('log(bp)')\n",
    "cdsF = np.log10([x[1] for x in fesFeats])\n",
    "cdsS = np.log10([x[1] for x in otherFeats])\n",
    "cdsD = np.log10([x[1] for x in dupFeats])\n",
    "\n",
    "boxes = ax1.boxplot([cdsS,cdsF,cdsD],labels=['Singleton','Fast-evolving singleton','Duplicable'],patch_artist=True,flierprops={'ms':1})\n",
    "boxes['boxes'][0].set_fc('#DC3220')\n",
    "boxes['boxes'][1].set_fc('#6c35b5')\n",
    "boxes['boxes'][2].set_fc('#005AB5')\n",
    "for m in boxes['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "for tick in ax1.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    tick.set_ha('right')\n",
    "\n",
    "ax2.set_title('Expression')\n",
    "ax2.set_ylabel('log(TPM)')\n",
    "\n",
    "expF = np.log10([x[3] for x in fesFeats if x[3] is not None])\n",
    "expS = np.log10([x[3] for x in otherFeats if x[3] is not None])\n",
    "expD = np.log10([x[3] for x in dupFeats if x[3] is not None])\n",
    "    \n",
    "boxes = ax2.boxplot([expS,expF,expD],labels=['Singleton','Fast-evolving singleton','Duplicable'],patch_artist=True,flierprops={'ms':1})\n",
    "boxes['boxes'][0].set_fc('#DC3220')\n",
    "boxes['boxes'][1].set_fc('#6c35b5')\n",
    "boxes['boxes'][2].set_fc('#005AB5')\n",
    "for m in boxes['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "for tick in ax2.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    tick.set_ha('right')\n",
    "\n",
    "ax3.set_title('% GC3')\n",
    "ax3.set_ylabel('%')\n",
    "gcF = [x[2]*100 for x in fesFeats]\n",
    "gcS = [x[2]*100 for x in otherFeats]\n",
    "gcD = [x[2]*100 for x in dupFeats]\n",
    "    \n",
    "boxes = ax3.boxplot([gcS,gcF,gcD],labels=['Singleton','Fast-evolving singleton','Duplicable'],patch_artist=True,flierprops={'ms':1})\n",
    "boxes['boxes'][0].set_fc('#DC3220')\n",
    "boxes['boxes'][1].set_fc('#6c35b5')\n",
    "boxes['boxes'][2].set_fc('#005AB5')\n",
    "for m in boxes['medians']:\n",
    "            m.set_color('k')\n",
    "            m.set_lw(2)\n",
    "for tick in ax3.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    tick.set_ha('right')\n",
    "plt.savefig('fesConfound_final.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abe4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxRestrictionLevels(defFES('procTrees_Orthofinder'),'singTrees_Orthofinder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d32399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50d90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
